{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a94d1da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5572f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_excel('politeness_2k_data.xlsx')\n",
    "data = data.fillna('_NA_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc1954fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"target\"]\n",
    "y_train = data[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76985f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data['doc_len'] = data['comment'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(data['doc_len'].mean() + data['doc_len'].std()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7708ac94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/amiangshu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      " 16%|█▌        | 326/2066 [00:00<00:00, 3244.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2066/2066 [00:00<00:00, 4779.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "\n",
    "\n",
    "## preprocessing starting \n",
    "raw_docs_train = data['comment'].tolist()\n",
    "num_classes = len(label_names)\n",
    "\n",
    "print(\"pre-processing train data...\")\n",
    "\n",
    "processed_docs_train = []\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = word_tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_docs_train.append(\" \".join(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb10eb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n",
      "dictionary size:  7245\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "MAX_NB_WORDS = 10000\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "print(\"tokenizing input data...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs_train )  #leaky\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "#pad sequences\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1da55743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3894it [00:00, 19477.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000005it [01:48, 18455.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1999997 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('loading word embeddings...')\n",
    "import os, re, csv, math, codecs\n",
    "\n",
    "embeddings_index = {}\n",
    "f = codecs.open('Embedding/crawl-300d-2M.vec', encoding='utf-8')\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6bf056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 1245\n"
     ]
    }
   ],
   "source": [
    "#embedding matrix\n",
    "\n",
    "print('preparing embedding matrix...')\n",
    "embed_dim = 300\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "095bd354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=125)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import  precision_score\n",
    "from sklearn.metrics import  f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63904eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#only LSTM\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#max_features =22248\n",
    "#nb_words=22248\n",
    "embedding_dim =300\n",
    "sequence_length = 100\n",
    "\n",
    "def LSTM_model():\n",
    "    model =  keras.Sequential()\n",
    "    #model.add(tf.keras.layers.Embedding(max_features +1, embedding_dim, input_length=sequence_length,\\\n",
    "                                    #embeddings_regularizer = regularizers.l2(0.005))) \n",
    "    model.add( keras.layers.Embedding(nb_words,embed_dim,input_length=max_seq_len,weights=[embedding_matrix],trainable=False))\n",
    "    model.add( keras.layers.Dropout(0.4))\n",
    "\n",
    "    model.add( keras.layers.LSTM(embedding_dim,dropout=0.2, recurrent_dropout=0.2,return_sequences=True,\\\n",
    "                                                             kernel_regularizer=regularizers.l2(0.005),\\\n",
    "                                                             bias_regularizer=regularizers.l2(0.005)))\n",
    "\n",
    "    model.add( keras.layers.Flatten())\n",
    "\n",
    "    model.add( keras.layers.Dense(512, activation='relu',\\\n",
    "                                kernel_regularizer=regularizers.l2(0.001),\\\n",
    "                                bias_regularizer=regularizers.l2(0.001),))\n",
    "    model.add( keras.layers.Dropout(0.4))\n",
    "\n",
    "    model.add( keras.layers.Dense(8, activation='relu',\\\n",
    "                                kernel_regularizer=regularizers.l2(0.001),\\\n",
    "                                bias_regularizer=regularizers.l2(0.001),))\n",
    "    model.add( keras.layers.Dropout(0.4))\n",
    "\n",
    "\n",
    "    model.add( keras.layers.Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=tf.keras.optimizers.Adam(1e-3),metrics=['acc'])\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38db7ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/amiangshu/anaconda3/envs/tf-bosu/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 57, 300)           2173800   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 57, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 57, 300)           721200    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 17100)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               8755712   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 4104      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 11,654,825\n",
      "Trainable params: 9,481,025\n",
      "Non-trainable params: 2,173,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM_model().summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b335fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "#plot_model(LSTM_model(), to_file='LSTMmodel.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73cc7c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping(monitor='val_loss', patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cb04df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_run_precision = []\n",
    "lstm_run_recall = []\n",
    "lstm_run_f1score = []\n",
    "lstm_run_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5f9ac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  1\n",
      "Train on 1652 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1652/1652 [==============================] - 1s 902us/sample - loss: 5.1032 - acc: 0.7100 - val_loss: 4.3410 - val_acc: 0.8019\n",
      "Epoch 2/40\n",
      "1652/1652 [==============================] - 0s 218us/sample - loss: 4.0463 - acc: 0.8154 - val_loss: 3.5545 - val_acc: 0.8019\n",
      "Epoch 3/40\n",
      "1652/1652 [==============================] - 0s 222us/sample - loss: 3.3733 - acc: 0.8154 - val_loss: 3.0659 - val_acc: 0.8019\n",
      "Epoch 4/40\n",
      "1652/1652 [==============================] - 0s 221us/sample - loss: 2.8874 - acc: 0.8160 - val_loss: 2.6460 - val_acc: 0.8019\n",
      "Epoch 5/40\n",
      "1652/1652 [==============================] - 0s 221us/sample - loss: 2.5572 - acc: 0.8184 - val_loss: 2.3860 - val_acc: 0.8019\n",
      "Epoch 6/40\n",
      "1652/1652 [==============================] - 0s 220us/sample - loss: 2.3503 - acc: 0.8172 - val_loss: 2.2243 - val_acc: 0.8068\n",
      "Epoch 7/40\n",
      "1652/1652 [==============================] - 0s 216us/sample - loss: 2.1739 - acc: 0.8226 - val_loss: 2.0953 - val_acc: 0.8068\n",
      "Epoch 8/40\n",
      "1652/1652 [==============================] - 0s 221us/sample - loss: 2.0521 - acc: 0.8456 - val_loss: 1.9969 - val_acc: 0.8116\n",
      "Epoch 9/40\n",
      "1652/1652 [==============================] - 0s 221us/sample - loss: 1.9762 - acc: 0.8450 - val_loss: 1.9347 - val_acc: 0.8116\n",
      "Epoch 10/40\n",
      "1652/1652 [==============================] - 0s 223us/sample - loss: 1.9061 - acc: 0.8620 - val_loss: 1.8814 - val_acc: 0.8261\n",
      "Epoch 11/40\n",
      "1652/1652 [==============================] - 0s 223us/sample - loss: 1.8753 - acc: 0.8596 - val_loss: 1.8490 - val_acc: 0.8309\n",
      "Epoch 12/40\n",
      "1652/1652 [==============================] - 0s 221us/sample - loss: 1.8382 - acc: 0.8674 - val_loss: 1.8693 - val_acc: 0.8068\n",
      "Epoch 13/40\n",
      "1652/1652 [==============================] - 0s 225us/sample - loss: 1.8160 - acc: 0.8596 - val_loss: 1.8153 - val_acc: 0.8164\n",
      "Epoch 14/40\n",
      "1652/1652 [==============================] - 0s 221us/sample - loss: 1.7531 - acc: 0.8814 - val_loss: 1.7941 - val_acc: 0.8116\n",
      "Epoch 15/40\n",
      "1652/1652 [==============================] - 0s 221us/sample - loss: 1.7227 - acc: 0.8801 - val_loss: 1.7707 - val_acc: 0.8309\n",
      "Epoch 16/40\n",
      "1652/1652 [==============================] - 0s 225us/sample - loss: 1.7024 - acc: 0.8874 - val_loss: 1.7581 - val_acc: 0.8213\n",
      "Epoch 17/40\n",
      "1652/1652 [==============================] - 0s 224us/sample - loss: 1.6917 - acc: 0.8759 - val_loss: 1.7407 - val_acc: 0.8213\n",
      "Epoch 18/40\n",
      "1652/1652 [==============================] - 0s 219us/sample - loss: 1.6692 - acc: 0.8638 - val_loss: 1.6883 - val_acc: 0.8261\n",
      "Epoch 19/40\n",
      "1652/1652 [==============================] - 0s 222us/sample - loss: 1.6503 - acc: 0.8862 - val_loss: 1.6925 - val_acc: 0.8019\n",
      "Epoch 20/40\n",
      "1652/1652 [==============================] - 0s 220us/sample - loss: 1.6284 - acc: 0.8886 - val_loss: 1.6667 - val_acc: 0.8213\n",
      "Epoch 21/40\n",
      "1652/1652 [==============================] - 0s 227us/sample - loss: 1.6219 - acc: 0.8977 - val_loss: 1.6578 - val_acc: 0.8213\n",
      "Epoch 22/40\n",
      "1652/1652 [==============================] - 0s 218us/sample - loss: 1.6376 - acc: 0.8795 - val_loss: 1.6615 - val_acc: 0.8164\n",
      "Epoch 23/40\n",
      "1652/1652 [==============================] - 0s 223us/sample - loss: 1.6526 - acc: 0.8705 - val_loss: 1.6446 - val_acc: 0.8309\n",
      "Epoch 24/40\n",
      "1652/1652 [==============================] - 0s 218us/sample - loss: 1.6128 - acc: 0.8862 - val_loss: 1.6781 - val_acc: 0.8164\n",
      "Epoch 25/40\n",
      "1652/1652 [==============================] - 0s 222us/sample - loss: 1.5799 - acc: 0.8898 - val_loss: 1.6496 - val_acc: 0.8116\n",
      "Epoch 26/40\n",
      "1652/1652 [==============================] - 0s 214us/sample - loss: 1.5598 - acc: 0.8959 - val_loss: 1.6058 - val_acc: 0.7971\n",
      "Epoch 27/40\n",
      "1652/1652 [==============================] - 0s 217us/sample - loss: 1.5145 - acc: 0.8923 - val_loss: 1.5838 - val_acc: 0.8213\n",
      "Epoch 28/40\n",
      "1652/1652 [==============================] - 0s 220us/sample - loss: 1.4800 - acc: 0.9031 - val_loss: 1.5956 - val_acc: 0.8213\n",
      "Epoch 29/40\n",
      "1652/1652 [==============================] - 0s 219us/sample - loss: 1.4569 - acc: 0.9025 - val_loss: 1.5604 - val_acc: 0.8019\n",
      "Epoch 30/40\n",
      "1652/1652 [==============================] - 0s 222us/sample - loss: 1.4277 - acc: 0.9116 - val_loss: 1.5486 - val_acc: 0.8116\n",
      "Epoch 31/40\n",
      "1652/1652 [==============================] - 0s 225us/sample - loss: 1.4268 - acc: 0.8941 - val_loss: 1.5335 - val_acc: 0.8213\n",
      "Epoch 32/40\n",
      "1652/1652 [==============================] - 0s 233us/sample - loss: 1.3977 - acc: 0.9050 - val_loss: 1.5486 - val_acc: 0.7971\n",
      "Epoch 33/40\n",
      "1652/1652 [==============================] - 0s 226us/sample - loss: 1.3815 - acc: 0.9068 - val_loss: 1.4667 - val_acc: 0.8261\n",
      "Epoch 34/40\n",
      "1652/1652 [==============================] - 0s 219us/sample - loss: 1.3822 - acc: 0.9025 - val_loss: 1.4715 - val_acc: 0.8019\n",
      "Epoch 35/40\n",
      "1652/1652 [==============================] - 0s 223us/sample - loss: 1.3368 - acc: 0.9110 - val_loss: 1.4281 - val_acc: 0.8406\n",
      "Epoch 36/40\n",
      "1652/1652 [==============================] - 0s 214us/sample - loss: 1.3305 - acc: 0.9153 - val_loss: 1.5008 - val_acc: 0.8019\n",
      "Epoch 37/40\n",
      "1652/1652 [==============================] - 0s 215us/sample - loss: 1.3276 - acc: 0.8971 - val_loss: 1.4593 - val_acc: 0.7971\n",
      "Epoch 38/40\n",
      "1652/1652 [==============================] - 0s 219us/sample - loss: 1.3048 - acc: 0.9001 - val_loss: 1.3907 - val_acc: 0.8164\n",
      "Epoch 39/40\n",
      "1652/1652 [==============================] - 0s 217us/sample - loss: 1.2877 - acc: 0.9207 - val_loss: 1.3972 - val_acc: 0.8164\n",
      "Epoch 40/40\n",
      "1652/1652 [==============================] - 0s 210us/sample - loss: 1.2529 - acc: 0.9159 - val_loss: 1.3942 - val_acc: 0.8068\n",
      "Train: 0.969, Test: 0.807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89       165\n",
      "           1       0.58      0.26      0.36        42\n",
      "\n",
      "    accuracy                           0.81       207\n",
      "   macro avg       0.71      0.61      0.63       207\n",
      "weighted avg       0.78      0.81      0.78       207\n",
      "\n",
      "\n",
      "Fold  2\n",
      "Train on 1652 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1652/1652 [==============================] - 2s 990us/sample - loss: 5.0234 - acc: 0.7409 - val_loss: 4.1909 - val_acc: 0.8164\n",
      "Epoch 2/40\n",
      "1652/1652 [==============================] - 0s 218us/sample - loss: 3.8627 - acc: 0.8087 - val_loss: 3.3619 - val_acc: 0.8164\n",
      "Epoch 3/40\n",
      "1652/1652 [==============================] - 0s 215us/sample - loss: 3.1911 - acc: 0.8093 - val_loss: 2.8672 - val_acc: 0.8164\n",
      "Epoch 4/40\n",
      "1652/1652 [==============================] - 0s 220us/sample - loss: 2.7418 - acc: 0.8087 - val_loss: 2.5093 - val_acc: 0.8164\n",
      "Epoch 5/40\n",
      "1652/1652 [==============================] - 0s 227us/sample - loss: 2.4455 - acc: 0.8099 - val_loss: 2.2824 - val_acc: 0.8164\n",
      "Epoch 6/40\n",
      "1652/1652 [==============================] - 0s 213us/sample - loss: 2.2248 - acc: 0.8202 - val_loss: 2.1164 - val_acc: 0.8213\n",
      "Epoch 7/40\n",
      "1652/1652 [==============================] - 0s 218us/sample - loss: 2.0837 - acc: 0.8238 - val_loss: 2.0066 - val_acc: 0.8116\n",
      "Epoch 8/40\n",
      "1652/1652 [==============================] - 0s 217us/sample - loss: 1.9972 - acc: 0.8160 - val_loss: 1.9372 - val_acc: 0.8019\n",
      "Epoch 9/40\n",
      "1652/1652 [==============================] - 0s 215us/sample - loss: 1.9205 - acc: 0.8214 - val_loss: 1.8946 - val_acc: 0.8068\n",
      "Epoch 10/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 1.8676 - acc: 0.8281 - val_loss: 1.8306 - val_acc: 0.8164\n",
      "Epoch 11/40\n",
      "1652/1652 [==============================] - 0s 218us/sample - loss: 1.8029 - acc: 0.8360 - val_loss: 1.8063 - val_acc: 0.8116\n",
      "Epoch 12/40\n",
      "1652/1652 [==============================] - 0s 213us/sample - loss: 1.7939 - acc: 0.8378 - val_loss: 1.7794 - val_acc: 0.8213\n",
      "Epoch 13/40\n",
      "1652/1652 [==============================] - 0s 216us/sample - loss: 1.7615 - acc: 0.8438 - val_loss: 1.7591 - val_acc: 0.8357\n",
      "Epoch 14/40\n",
      "1652/1652 [==============================] - 0s 219us/sample - loss: 1.7481 - acc: 0.8450 - val_loss: 1.7469 - val_acc: 0.8213\n",
      "Epoch 15/40\n",
      "1652/1652 [==============================] - 0s 214us/sample - loss: 1.6921 - acc: 0.8547 - val_loss: 1.7340 - val_acc: 0.8309\n",
      "Epoch 16/40\n",
      "1652/1652 [==============================] - 0s 220us/sample - loss: 1.6852 - acc: 0.8511 - val_loss: 1.7107 - val_acc: 0.8309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40\n",
      "1652/1652 [==============================] - 0s 217us/sample - loss: 1.6792 - acc: 0.8438 - val_loss: 1.6899 - val_acc: 0.8357\n",
      "Epoch 18/40\n",
      "1652/1652 [==============================] - 0s 209us/sample - loss: 1.6344 - acc: 0.8686 - val_loss: 1.6645 - val_acc: 0.8406\n",
      "Epoch 19/40\n",
      "1652/1652 [==============================] - 0s 215us/sample - loss: 1.6160 - acc: 0.8571 - val_loss: 1.6715 - val_acc: 0.8261\n",
      "Epoch 20/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 1.6184 - acc: 0.8499 - val_loss: 1.6356 - val_acc: 0.8357\n",
      "Epoch 21/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 1.6022 - acc: 0.8535 - val_loss: 1.6578 - val_acc: 0.8019\n",
      "Epoch 22/40\n",
      "1652/1652 [==============================] - 0s 213us/sample - loss: 1.5719 - acc: 0.8571 - val_loss: 1.6233 - val_acc: 0.8213\n",
      "Epoch 23/40\n",
      "1652/1652 [==============================] - 0s 210us/sample - loss: 1.5389 - acc: 0.8656 - val_loss: 1.5995 - val_acc: 0.8454\n",
      "Epoch 24/40\n",
      "1652/1652 [==============================] - 0s 212us/sample - loss: 1.5289 - acc: 0.8638 - val_loss: 1.5729 - val_acc: 0.8309\n",
      "Epoch 25/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 1.5231 - acc: 0.8541 - val_loss: 1.5384 - val_acc: 0.8502\n",
      "Epoch 26/40\n",
      "1652/1652 [==============================] - 0s 210us/sample - loss: 1.4655 - acc: 0.8717 - val_loss: 1.5373 - val_acc: 0.8357\n",
      "Epoch 27/40\n",
      "1652/1652 [==============================] - 0s 210us/sample - loss: 1.4541 - acc: 0.8777 - val_loss: 1.4984 - val_acc: 0.8502\n",
      "Epoch 28/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 1.4336 - acc: 0.8723 - val_loss: 1.4756 - val_acc: 0.8454\n",
      "Epoch 29/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 1.4105 - acc: 0.8923 - val_loss: 1.4562 - val_acc: 0.8599\n",
      "Epoch 30/40\n",
      "1652/1652 [==============================] - 0s 208us/sample - loss: 1.4056 - acc: 0.8795 - val_loss: 1.4703 - val_acc: 0.8261\n",
      "Epoch 31/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 1.4165 - acc: 0.8602 - val_loss: 1.4212 - val_acc: 0.8502\n",
      "Epoch 32/40\n",
      "1652/1652 [==============================] - 0s 213us/sample - loss: 1.3979 - acc: 0.8729 - val_loss: 1.4502 - val_acc: 0.8502\n",
      "Epoch 33/40\n",
      "1652/1652 [==============================] - 0s 210us/sample - loss: 1.3455 - acc: 0.8916 - val_loss: 1.4416 - val_acc: 0.8406\n",
      "Epoch 34/40\n",
      "1652/1652 [==============================] - 0s 213us/sample - loss: 1.3446 - acc: 0.8844 - val_loss: 1.4222 - val_acc: 0.8261\n",
      "Train: 0.935, Test: 0.826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91       172\n",
      "           1       0.55      0.34      0.42        35\n",
      "\n",
      "    accuracy                           0.84       207\n",
      "   macro avg       0.71      0.64      0.66       207\n",
      "weighted avg       0.82      0.84      0.83       207\n",
      "\n",
      "\n",
      "Fold  3\n",
      "Train on 1652 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1652/1652 [==============================] - 2s 1ms/sample - loss: 5.0562 - acc: 0.7621 - val_loss: 4.3367 - val_acc: 0.8261\n",
      "Epoch 2/40\n",
      "1652/1652 [==============================] - 0s 233us/sample - loss: 4.0432 - acc: 0.8081 - val_loss: 3.5052 - val_acc: 0.8261\n",
      "Epoch 3/40\n",
      "1652/1652 [==============================] - 0s 227us/sample - loss: 3.3370 - acc: 0.8087 - val_loss: 3.0117 - val_acc: 0.8261\n",
      "Epoch 4/40\n",
      "1652/1652 [==============================] - 0s 220us/sample - loss: 2.8454 - acc: 0.8093 - val_loss: 2.6021 - val_acc: 0.8261\n",
      "Epoch 5/40\n",
      "1652/1652 [==============================] - 0s 225us/sample - loss: 2.5083 - acc: 0.8117 - val_loss: 2.3744 - val_acc: 0.8357\n",
      "Epoch 6/40\n",
      "1652/1652 [==============================] - 0s 234us/sample - loss: 2.2973 - acc: 0.8257 - val_loss: 2.2018 - val_acc: 0.8309\n",
      "Epoch 7/40\n",
      "1652/1652 [==============================] - 0s 233us/sample - loss: 2.1607 - acc: 0.8202 - val_loss: 2.0621 - val_acc: 0.8454\n",
      "Epoch 8/40\n",
      "1652/1652 [==============================] - 0s 226us/sample - loss: 2.0398 - acc: 0.8360 - val_loss: 1.9772 - val_acc: 0.8164\n",
      "Epoch 9/40\n",
      "1652/1652 [==============================] - 0s 231us/sample - loss: 1.9449 - acc: 0.8462 - val_loss: 1.9092 - val_acc: 0.8406\n",
      "Epoch 10/40\n",
      "1652/1652 [==============================] - 0s 222us/sample - loss: 1.8828 - acc: 0.8541 - val_loss: 1.8586 - val_acc: 0.8454\n",
      "Epoch 11/40\n",
      "1652/1652 [==============================] - 0s 218us/sample - loss: 1.8373 - acc: 0.8517 - val_loss: 1.8183 - val_acc: 0.8696\n",
      "Epoch 12/40\n",
      "1652/1652 [==============================] - 0s 220us/sample - loss: 1.7940 - acc: 0.8577 - val_loss: 1.7796 - val_acc: 0.8599\n",
      "Epoch 13/40\n",
      "1652/1652 [==============================] - 0s 224us/sample - loss: 1.7570 - acc: 0.8644 - val_loss: 1.7879 - val_acc: 0.8647\n",
      "Epoch 14/40\n",
      "1652/1652 [==============================] - 0s 230us/sample - loss: 1.7215 - acc: 0.8741 - val_loss: 1.7803 - val_acc: 0.8406\n",
      "Epoch 15/40\n",
      "1652/1652 [==============================] - 0s 229us/sample - loss: 1.7123 - acc: 0.8656 - val_loss: 1.7159 - val_acc: 0.8357\n",
      "Epoch 16/40\n",
      "1652/1652 [==============================] - 0s 221us/sample - loss: 1.6698 - acc: 0.8765 - val_loss: 1.6936 - val_acc: 0.8551\n",
      "Epoch 17/40\n",
      "1652/1652 [==============================] - 0s 224us/sample - loss: 1.6302 - acc: 0.8765 - val_loss: 1.6762 - val_acc: 0.8599\n",
      "Epoch 18/40\n",
      "1652/1652 [==============================] - 0s 230us/sample - loss: 1.6283 - acc: 0.8620 - val_loss: 1.6536 - val_acc: 0.8406\n",
      "Epoch 19/40\n",
      "1652/1652 [==============================] - 0s 232us/sample - loss: 1.6011 - acc: 0.8741 - val_loss: 1.6457 - val_acc: 0.8696\n",
      "Epoch 20/40\n",
      "1652/1652 [==============================] - 0s 229us/sample - loss: 1.5844 - acc: 0.8868 - val_loss: 1.6380 - val_acc: 0.8357\n",
      "Epoch 21/40\n",
      "1652/1652 [==============================] - 0s 217us/sample - loss: 1.5588 - acc: 0.8850 - val_loss: 1.6371 - val_acc: 0.8309\n",
      "Epoch 22/40\n",
      "1652/1652 [==============================] - 0s 226us/sample - loss: 1.5453 - acc: 0.8795 - val_loss: 1.5932 - val_acc: 0.8357\n",
      "Epoch 23/40\n",
      "1652/1652 [==============================] - 0s 229us/sample - loss: 1.5198 - acc: 0.8808 - val_loss: 1.5903 - val_acc: 0.8357\n",
      "Epoch 24/40\n",
      "1652/1652 [==============================] - 0s 231us/sample - loss: 1.4966 - acc: 0.8898 - val_loss: 1.5420 - val_acc: 0.8647\n",
      "Epoch 25/40\n",
      "1652/1652 [==============================] - 0s 218us/sample - loss: 1.4879 - acc: 0.8874 - val_loss: 1.5239 - val_acc: 0.8502\n",
      "Epoch 26/40\n",
      "1652/1652 [==============================] - 0s 216us/sample - loss: 1.4607 - acc: 0.8971 - val_loss: 1.5615 - val_acc: 0.8116\n",
      "Epoch 27/40\n",
      "1652/1652 [==============================] - 0s 221us/sample - loss: 1.4491 - acc: 0.8892 - val_loss: 1.5131 - val_acc: 0.8696\n",
      "Epoch 28/40\n",
      "1652/1652 [==============================] - 0s 226us/sample - loss: 1.4216 - acc: 0.9074 - val_loss: 1.4755 - val_acc: 0.8599\n",
      "Epoch 29/40\n",
      "1652/1652 [==============================] - 0s 219us/sample - loss: 1.3922 - acc: 0.8971 - val_loss: 1.4844 - val_acc: 0.8502\n",
      "Epoch 30/40\n",
      "1652/1652 [==============================] - 0s 228us/sample - loss: 1.3978 - acc: 0.8910 - val_loss: 1.4532 - val_acc: 0.8502\n",
      "Epoch 31/40\n",
      "1652/1652 [==============================] - 0s 226us/sample - loss: 1.3598 - acc: 0.8959 - val_loss: 1.5137 - val_acc: 0.8406\n",
      "Epoch 32/40\n",
      "1652/1652 [==============================] - 0s 222us/sample - loss: 1.3694 - acc: 0.8977 - val_loss: 1.4419 - val_acc: 0.8357\n",
      "Epoch 33/40\n",
      "1652/1652 [==============================] - 0s 237us/sample - loss: 1.3408 - acc: 0.8929 - val_loss: 1.3991 - val_acc: 0.8551\n",
      "Epoch 34/40\n",
      "1652/1652 [==============================] - 0s 225us/sample - loss: 1.3161 - acc: 0.9001 - val_loss: 1.3955 - val_acc: 0.8696\n",
      "Epoch 35/40\n",
      "1652/1652 [==============================] - 0s 230us/sample - loss: 1.2773 - acc: 0.9140 - val_loss: 1.4315 - val_acc: 0.8357\n",
      "Epoch 36/40\n",
      "1652/1652 [==============================] - 0s 230us/sample - loss: 1.2879 - acc: 0.9019 - val_loss: 1.4555 - val_acc: 0.8068\n",
      "Epoch 37/40\n",
      "1652/1652 [==============================] - 0s 219us/sample - loss: 1.2639 - acc: 0.9001 - val_loss: 1.3698 - val_acc: 0.8502\n",
      "Epoch 38/40\n",
      "1652/1652 [==============================] - 0s 217us/sample - loss: 1.2843 - acc: 0.9007 - val_loss: 1.3657 - val_acc: 0.8551\n",
      "Epoch 39/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1652/1652 [==============================] - 0s 219us/sample - loss: 1.2554 - acc: 0.9019 - val_loss: 1.3665 - val_acc: 0.8406\n",
      "Epoch 40/40\n",
      "1652/1652 [==============================] - 0s 213us/sample - loss: 1.2126 - acc: 0.9098 - val_loss: 1.3415 - val_acc: 0.8502\n",
      "Train: 0.960, Test: 0.850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92       171\n",
      "           1       0.71      0.28      0.40        36\n",
      "\n",
      "    accuracy                           0.86       207\n",
      "   macro avg       0.79      0.63      0.66       207\n",
      "weighted avg       0.84      0.86      0.83       207\n",
      "\n",
      "\n",
      "Fold  4\n",
      "Train on 1652 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1652/1652 [==============================] - 2s 1ms/sample - loss: 5.0608 - acc: 0.7500 - val_loss: 4.3900 - val_acc: 0.7826\n",
      "Epoch 2/40\n",
      "1652/1652 [==============================] - 0s 222us/sample - loss: 4.0273 - acc: 0.8220 - val_loss: 3.6127 - val_acc: 0.7826\n",
      "Epoch 3/40\n",
      "1652/1652 [==============================] - 0s 222us/sample - loss: 3.3168 - acc: 0.8245 - val_loss: 3.0880 - val_acc: 0.7923\n",
      "Epoch 4/40\n",
      "1652/1652 [==============================] - 0s 230us/sample - loss: 2.8570 - acc: 0.8305 - val_loss: 2.7219 - val_acc: 0.7874\n",
      "Epoch 5/40\n",
      "1652/1652 [==============================] - 0s 226us/sample - loss: 2.5518 - acc: 0.8335 - val_loss: 2.4804 - val_acc: 0.8019\n",
      "Epoch 6/40\n",
      "1652/1652 [==============================] - 0s 232us/sample - loss: 2.3172 - acc: 0.8529 - val_loss: 2.3619 - val_acc: 0.7923\n",
      "Epoch 7/40\n",
      "1652/1652 [==============================] - 0s 225us/sample - loss: 2.1741 - acc: 0.8438 - val_loss: 2.2687 - val_acc: 0.8068\n",
      "Epoch 8/40\n",
      "1652/1652 [==============================] - 0s 226us/sample - loss: 2.0804 - acc: 0.8493 - val_loss: 2.1156 - val_acc: 0.7874\n",
      "Epoch 9/40\n",
      "1652/1652 [==============================] - 0s 230us/sample - loss: 1.9764 - acc: 0.8626 - val_loss: 2.0528 - val_acc: 0.7778\n",
      "Epoch 10/40\n",
      "1652/1652 [==============================] - 0s 227us/sample - loss: 1.9099 - acc: 0.8650 - val_loss: 2.0060 - val_acc: 0.8116\n",
      "Epoch 11/40\n",
      "1652/1652 [==============================] - 0s 222us/sample - loss: 1.8523 - acc: 0.8735 - val_loss: 1.9776 - val_acc: 0.8164\n",
      "Epoch 12/40\n",
      "1652/1652 [==============================] - 0s 224us/sample - loss: 1.7993 - acc: 0.8820 - val_loss: 2.0273 - val_acc: 0.8019\n",
      "Epoch 13/40\n",
      "1652/1652 [==============================] - 0s 225us/sample - loss: 1.7785 - acc: 0.8868 - val_loss: 1.9304 - val_acc: 0.8164\n",
      "Epoch 14/40\n",
      "1652/1652 [==============================] - 0s 227us/sample - loss: 1.7344 - acc: 0.8832 - val_loss: 1.9282 - val_acc: 0.7971\n",
      "Epoch 15/40\n",
      "1652/1652 [==============================] - 0s 227us/sample - loss: 1.7143 - acc: 0.8729 - val_loss: 1.9201 - val_acc: 0.8116\n",
      "Epoch 16/40\n",
      "1652/1652 [==============================] - 0s 222us/sample - loss: 1.6818 - acc: 0.9001 - val_loss: 1.9367 - val_acc: 0.8116\n",
      "Epoch 17/40\n",
      "1652/1652 [==============================] - 0s 225us/sample - loss: 1.6709 - acc: 0.8856 - val_loss: 1.8378 - val_acc: 0.8116\n",
      "Epoch 18/40\n",
      "1652/1652 [==============================] - 0s 227us/sample - loss: 1.6692 - acc: 0.8783 - val_loss: 1.8126 - val_acc: 0.7633\n",
      "Epoch 19/40\n",
      "1652/1652 [==============================] - 0s 218us/sample - loss: 1.6283 - acc: 0.8910 - val_loss: 1.7971 - val_acc: 0.8019\n",
      "Epoch 20/40\n",
      "1652/1652 [==============================] - 0s 226us/sample - loss: 1.5940 - acc: 0.8971 - val_loss: 1.8343 - val_acc: 0.8261\n",
      "Epoch 21/40\n",
      "1652/1652 [==============================] - 0s 225us/sample - loss: 1.5776 - acc: 0.8959 - val_loss: 1.7767 - val_acc: 0.8213\n",
      "Epoch 22/40\n",
      "1652/1652 [==============================] - 0s 227us/sample - loss: 1.5364 - acc: 0.9068 - val_loss: 1.7347 - val_acc: 0.8261\n",
      "Epoch 23/40\n",
      "1652/1652 [==============================] - 0s 221us/sample - loss: 1.5435 - acc: 0.8965 - val_loss: 1.7514 - val_acc: 0.7874\n",
      "Epoch 24/40\n",
      "1652/1652 [==============================] - 0s 224us/sample - loss: 1.5009 - acc: 0.9013 - val_loss: 1.7342 - val_acc: 0.7874\n",
      "Epoch 25/40\n",
      "1652/1652 [==============================] - 0s 226us/sample - loss: 1.4880 - acc: 0.9001 - val_loss: 1.6731 - val_acc: 0.8116\n",
      "Epoch 26/40\n",
      "1652/1652 [==============================] - 0s 226us/sample - loss: 1.5090 - acc: 0.9074 - val_loss: 1.6623 - val_acc: 0.8213\n",
      "Epoch 27/40\n",
      "1652/1652 [==============================] - 0s 225us/sample - loss: 1.4504 - acc: 0.9116 - val_loss: 1.7435 - val_acc: 0.7729\n",
      "Epoch 28/40\n",
      "1652/1652 [==============================] - 0s 228us/sample - loss: 1.4366 - acc: 0.9104 - val_loss: 1.6832 - val_acc: 0.7343\n",
      "Epoch 29/40\n",
      "1652/1652 [==============================] - 0s 223us/sample - loss: 1.4138 - acc: 0.9122 - val_loss: 1.6877 - val_acc: 0.7488\n",
      "Train: 0.944, Test: 0.749\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86       158\n",
      "           1       0.54      0.59      0.56        49\n",
      "\n",
      "    accuracy                           0.78       207\n",
      "   macro avg       0.70      0.72      0.71       207\n",
      "weighted avg       0.79      0.78      0.79       207\n",
      "\n",
      "\n",
      "Fold  5\n",
      "Train on 1652 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1652/1652 [==============================] - 2s 1ms/sample - loss: 5.1106 - acc: 0.7603 - val_loss: 4.4125 - val_acc: 0.7971\n",
      "Epoch 2/40\n",
      "1652/1652 [==============================] - 0s 219us/sample - loss: 4.0630 - acc: 0.8166 - val_loss: 3.5728 - val_acc: 0.7971\n",
      "Epoch 3/40\n",
      "1652/1652 [==============================] - 0s 215us/sample - loss: 3.3528 - acc: 0.8178 - val_loss: 3.0156 - val_acc: 0.8019\n",
      "Epoch 4/40\n",
      "1652/1652 [==============================] - 0s 215us/sample - loss: 2.8700 - acc: 0.8208 - val_loss: 2.6629 - val_acc: 0.8213\n",
      "Epoch 5/40\n",
      "1652/1652 [==============================] - 0s 218us/sample - loss: 2.5568 - acc: 0.8275 - val_loss: 2.3948 - val_acc: 0.8261\n",
      "Epoch 6/40\n",
      "1652/1652 [==============================] - 0s 213us/sample - loss: 2.3254 - acc: 0.8323 - val_loss: 2.2030 - val_acc: 0.8357\n",
      "Epoch 7/40\n",
      "1652/1652 [==============================] - 0s 213us/sample - loss: 2.1582 - acc: 0.8450 - val_loss: 2.0724 - val_acc: 0.8454\n",
      "Epoch 8/40\n",
      "1652/1652 [==============================] - 0s 220us/sample - loss: 2.0329 - acc: 0.8420 - val_loss: 1.9881 - val_acc: 0.8551\n",
      "Epoch 9/40\n",
      "1652/1652 [==============================] - 0s 218us/sample - loss: 1.9765 - acc: 0.8426 - val_loss: 1.9467 - val_acc: 0.8309\n",
      "Epoch 10/40\n",
      "1652/1652 [==============================] - 0s 224us/sample - loss: 1.9104 - acc: 0.8493 - val_loss: 1.8765 - val_acc: 0.8647\n",
      "Epoch 11/40\n",
      "1652/1652 [==============================] - 0s 223us/sample - loss: 1.8774 - acc: 0.8596 - val_loss: 1.8733 - val_acc: 0.8261\n",
      "Epoch 12/40\n",
      "1652/1652 [==============================] - 0s 222us/sample - loss: 1.8113 - acc: 0.8765 - val_loss: 1.8359 - val_acc: 0.8454\n",
      "Epoch 13/40\n",
      "1652/1652 [==============================] - 0s 220us/sample - loss: 1.7765 - acc: 0.8723 - val_loss: 1.7931 - val_acc: 0.8551\n",
      "Epoch 14/40\n",
      "1652/1652 [==============================] - 0s 228us/sample - loss: 1.7342 - acc: 0.8838 - val_loss: 1.7679 - val_acc: 0.8551\n",
      "Epoch 15/40\n",
      "1652/1652 [==============================] - 0s 221us/sample - loss: 1.7068 - acc: 0.8808 - val_loss: 1.7600 - val_acc: 0.8454\n",
      "Epoch 16/40\n",
      "1652/1652 [==============================] - 0s 215us/sample - loss: 1.6853 - acc: 0.8777 - val_loss: 1.7269 - val_acc: 0.8551\n",
      "Epoch 17/40\n",
      "1652/1652 [==============================] - 0s 229us/sample - loss: 1.6779 - acc: 0.8826 - val_loss: 1.7236 - val_acc: 0.8164\n",
      "Epoch 18/40\n",
      "1652/1652 [==============================] - 0s 221us/sample - loss: 1.6688 - acc: 0.8723 - val_loss: 1.7101 - val_acc: 0.8406\n",
      "Epoch 19/40\n",
      "1652/1652 [==============================] - 0s 227us/sample - loss: 1.6306 - acc: 0.8856 - val_loss: 1.7094 - val_acc: 0.8406\n",
      "Epoch 20/40\n",
      "1652/1652 [==============================] - 0s 224us/sample - loss: 1.5895 - acc: 0.8916 - val_loss: 1.7089 - val_acc: 0.8406\n",
      "Epoch 21/40\n",
      "1652/1652 [==============================] - 0s 219us/sample - loss: 1.5772 - acc: 0.8820 - val_loss: 1.6790 - val_acc: 0.8309\n",
      "Epoch 22/40\n",
      "1652/1652 [==============================] - 0s 223us/sample - loss: 1.5476 - acc: 0.8935 - val_loss: 1.6654 - val_acc: 0.8261\n",
      "Epoch 23/40\n",
      "1652/1652 [==============================] - 0s 217us/sample - loss: 1.5412 - acc: 0.8874 - val_loss: 1.6380 - val_acc: 0.7971\n",
      "Epoch 24/40\n",
      "1652/1652 [==============================] - 0s 218us/sample - loss: 1.5195 - acc: 0.8904 - val_loss: 1.6107 - val_acc: 0.8406\n",
      "Epoch 25/40\n",
      "1652/1652 [==============================] - 0s 215us/sample - loss: 1.5066 - acc: 0.9007 - val_loss: 1.6024 - val_acc: 0.8116\n",
      "Epoch 26/40\n",
      "1652/1652 [==============================] - 0s 209us/sample - loss: 1.4970 - acc: 0.8941 - val_loss: 1.6069 - val_acc: 0.8213\n",
      "Epoch 27/40\n",
      "1652/1652 [==============================] - 0s 214us/sample - loss: 1.4586 - acc: 0.9062 - val_loss: 1.5832 - val_acc: 0.8309\n",
      "Epoch 28/40\n",
      "1652/1652 [==============================] - 0s 215us/sample - loss: 1.5049 - acc: 0.9025 - val_loss: 1.5714 - val_acc: 0.8261\n",
      "Epoch 29/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 1.4635 - acc: 0.8844 - val_loss: 1.5896 - val_acc: 0.8309\n",
      "Epoch 30/40\n",
      "1652/1652 [==============================] - 0s 210us/sample - loss: 1.4813 - acc: 0.8765 - val_loss: 1.5638 - val_acc: 0.8406\n",
      "Epoch 31/40\n",
      "1652/1652 [==============================] - 0s 209us/sample - loss: 1.4614 - acc: 0.8916 - val_loss: 1.5916 - val_acc: 0.8164\n",
      "Epoch 32/40\n",
      "1652/1652 [==============================] - 0s 215us/sample - loss: 1.4283 - acc: 0.9086 - val_loss: 1.5760 - val_acc: 0.8213\n",
      "Epoch 33/40\n",
      "1652/1652 [==============================] - 0s 209us/sample - loss: 1.4123 - acc: 0.9062 - val_loss: 1.5053 - val_acc: 0.8357\n",
      "Epoch 34/40\n",
      "1652/1652 [==============================] - 0s 213us/sample - loss: 1.3629 - acc: 0.9110 - val_loss: 1.4794 - val_acc: 0.8454\n",
      "Epoch 35/40\n",
      "1652/1652 [==============================] - 0s 209us/sample - loss: 1.3224 - acc: 0.9062 - val_loss: 1.5660 - val_acc: 0.8116\n",
      "Epoch 36/40\n",
      "1652/1652 [==============================] - 0s 212us/sample - loss: 1.3374 - acc: 0.9080 - val_loss: 1.4607 - val_acc: 0.8068\n",
      "Epoch 37/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 1.3068 - acc: 0.9086 - val_loss: 1.4546 - val_acc: 0.7778\n",
      "Epoch 38/40\n",
      "1652/1652 [==============================] - 0s 209us/sample - loss: 1.3363 - acc: 0.8868 - val_loss: 1.4181 - val_acc: 0.8309\n",
      "Epoch 39/40\n",
      "1652/1652 [==============================] - 0s 209us/sample - loss: 1.2829 - acc: 0.9074 - val_loss: 1.4546 - val_acc: 0.8309\n",
      "Epoch 40/40\n",
      "1652/1652 [==============================] - 0s 214us/sample - loss: 1.2580 - acc: 0.9092 - val_loss: 1.3849 - val_acc: 0.8502\n",
      "Train: 0.960, Test: 0.850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.90       164\n",
      "           1       0.76      0.30      0.43        43\n",
      "\n",
      "    accuracy                           0.84       207\n",
      "   macro avg       0.80      0.64      0.67       207\n",
      "weighted avg       0.83      0.84      0.81       207\n",
      "\n",
      "\n",
      "Fold  6\n",
      "Train on 1652 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1652/1652 [==============================] - 2s 1ms/sample - loss: 5.0515 - acc: 0.7803 - val_loss: 4.4025 - val_acc: 0.8019\n",
      "Epoch 2/40\n",
      "1652/1652 [==============================] - 0s 210us/sample - loss: 4.0370 - acc: 0.8142 - val_loss: 3.5636 - val_acc: 0.8019\n",
      "Epoch 3/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 3.3374 - acc: 0.8142 - val_loss: 3.0474 - val_acc: 0.8019\n",
      "Epoch 4/40\n",
      "1652/1652 [==============================] - 0s 212us/sample - loss: 2.8712 - acc: 0.8148 - val_loss: 2.6904 - val_acc: 0.8019\n",
      "Epoch 5/40\n",
      "1652/1652 [==============================] - 0s 210us/sample - loss: 2.5415 - acc: 0.8142 - val_loss: 2.4177 - val_acc: 0.8019\n",
      "Epoch 6/40\n",
      "1652/1652 [==============================] - 0s 216us/sample - loss: 2.3207 - acc: 0.8148 - val_loss: 2.2507 - val_acc: 0.8019\n",
      "Epoch 7/40\n",
      "1652/1652 [==============================] - 0s 214us/sample - loss: 2.1463 - acc: 0.8148 - val_loss: 2.1248 - val_acc: 0.8019\n",
      "Epoch 8/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 2.0264 - acc: 0.8184 - val_loss: 2.0434 - val_acc: 0.8068\n",
      "Epoch 9/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 1.9551 - acc: 0.8269 - val_loss: 1.9579 - val_acc: 0.8068\n",
      "Epoch 10/40\n",
      "1652/1652 [==============================] - 0s 209us/sample - loss: 1.8834 - acc: 0.8226 - val_loss: 1.9151 - val_acc: 0.8068\n",
      "Epoch 11/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 1.8392 - acc: 0.8366 - val_loss: 1.9073 - val_acc: 0.8116\n",
      "Epoch 12/40\n",
      "1652/1652 [==============================] - 0s 213us/sample - loss: 1.8091 - acc: 0.8529 - val_loss: 1.9428 - val_acc: 0.8019\n",
      "Epoch 13/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 1.7822 - acc: 0.8547 - val_loss: 1.8237 - val_acc: 0.8261\n",
      "Epoch 14/40\n",
      "1652/1652 [==============================] - 0s 212us/sample - loss: 1.7393 - acc: 0.8674 - val_loss: 1.8452 - val_acc: 0.8116\n",
      "Epoch 15/40\n",
      "1652/1652 [==============================] - 0s 212us/sample - loss: 1.7205 - acc: 0.8644 - val_loss: 1.8048 - val_acc: 0.8309\n",
      "Epoch 16/40\n",
      "1652/1652 [==============================] - 0s 212us/sample - loss: 1.7006 - acc: 0.8686 - val_loss: 1.7971 - val_acc: 0.8261\n",
      "Epoch 17/40\n",
      "1652/1652 [==============================] - 0s 210us/sample - loss: 1.6566 - acc: 0.8777 - val_loss: 1.7889 - val_acc: 0.8357\n",
      "Epoch 18/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 1.6303 - acc: 0.8838 - val_loss: 1.7694 - val_acc: 0.8309\n",
      "Epoch 19/40\n",
      "1652/1652 [==============================] - 0s 209us/sample - loss: 1.5885 - acc: 0.8892 - val_loss: 1.8130 - val_acc: 0.8116\n",
      "Epoch 20/40\n",
      "1652/1652 [==============================] - 0s 211us/sample - loss: 1.5891 - acc: 0.8795 - val_loss: 1.8103 - val_acc: 0.8068\n",
      "Epoch 21/40\n",
      "1652/1652 [==============================] - 0s 214us/sample - loss: 1.5860 - acc: 0.8777 - val_loss: 1.7002 - val_acc: 0.8068\n",
      "Epoch 22/40\n",
      "1652/1652 [==============================] - 0s 207us/sample - loss: 1.5464 - acc: 0.8916 - val_loss: 1.6959 - val_acc: 0.8068\n",
      "Epoch 23/40\n",
      "1652/1652 [==============================] - 0s 209us/sample - loss: 1.5109 - acc: 0.8977 - val_loss: 1.6561 - val_acc: 0.8213\n",
      "Epoch 24/40\n",
      "1652/1652 [==============================] - 0s 210us/sample - loss: 1.4944 - acc: 0.8880 - val_loss: 1.6779 - val_acc: 0.8068\n",
      "Epoch 25/40\n",
      "1652/1652 [==============================] - 0s 209us/sample - loss: 1.4568 - acc: 0.9074 - val_loss: 1.7170 - val_acc: 0.8116\n",
      "Epoch 26/40\n",
      "1652/1652 [==============================] - 0s 210us/sample - loss: 1.4593 - acc: 0.8941 - val_loss: 1.6953 - val_acc: 0.8164\n",
      "Train: 0.949, Test: 0.816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.89       167\n",
      "           1       0.50      0.28      0.35        40\n",
      "\n",
      "    accuracy                           0.81       207\n",
      "   macro avg       0.67      0.60      0.62       207\n",
      "weighted avg       0.78      0.81      0.78       207\n",
      "\n",
      "\n",
      "Fold  7\n",
      "Train on 1653 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1653/1653 [==============================] - 2s 1ms/sample - loss: 5.0935 - acc: 0.7858 - val_loss: 4.3614 - val_acc: 0.8261\n",
      "Epoch 2/40\n",
      "1653/1653 [==============================] - 0s 214us/sample - loss: 4.0756 - acc: 0.8106 - val_loss: 3.5632 - val_acc: 0.8261\n",
      "Epoch 3/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 3.3734 - acc: 0.8106 - val_loss: 3.0543 - val_acc: 0.8261\n",
      "Epoch 4/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 2.8908 - acc: 0.8113 - val_loss: 2.6556 - val_acc: 0.8261\n",
      "Epoch 5/40\n",
      "1653/1653 [==============================] - 0s 209us/sample - loss: 2.5397 - acc: 0.8179 - val_loss: 2.4362 - val_acc: 0.8309\n",
      "Epoch 6/40\n",
      "1653/1653 [==============================] - 0s 208us/sample - loss: 2.3292 - acc: 0.8282 - val_loss: 2.2406 - val_acc: 0.8357\n",
      "Epoch 7/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 2.1645 - acc: 0.8240 - val_loss: 2.1185 - val_acc: 0.8406\n",
      "Epoch 8/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 2.0453 - acc: 0.8403 - val_loss: 2.0486 - val_acc: 0.8502\n",
      "Epoch 9/40\n",
      "1653/1653 [==============================] - 0s 210us/sample - loss: 1.9499 - acc: 0.8518 - val_loss: 2.0062 - val_acc: 0.8502\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1653/1653 [==============================] - 0s 215us/sample - loss: 1.8902 - acc: 0.8572 - val_loss: 1.9748 - val_acc: 0.8502\n",
      "Epoch 11/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.8446 - acc: 0.8621 - val_loss: 1.9153 - val_acc: 0.8502\n",
      "Epoch 12/40\n",
      "1653/1653 [==============================] - 0s 209us/sample - loss: 1.7841 - acc: 0.8766 - val_loss: 1.9448 - val_acc: 0.8406\n",
      "Epoch 13/40\n",
      "1653/1653 [==============================] - 0s 215us/sample - loss: 1.7708 - acc: 0.8730 - val_loss: 1.8976 - val_acc: 0.8406\n",
      "Epoch 14/40\n",
      "1653/1653 [==============================] - 0s 215us/sample - loss: 1.7428 - acc: 0.8675 - val_loss: 1.8206 - val_acc: 0.8406\n",
      "Epoch 15/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 1.7074 - acc: 0.8724 - val_loss: 1.8153 - val_acc: 0.8116\n",
      "Epoch 16/40\n",
      "1653/1653 [==============================] - 0s 214us/sample - loss: 1.6763 - acc: 0.8832 - val_loss: 1.8180 - val_acc: 0.8406\n",
      "Epoch 17/40\n",
      "1653/1653 [==============================] - 0s 210us/sample - loss: 1.6362 - acc: 0.8966 - val_loss: 1.8051 - val_acc: 0.8261\n",
      "Epoch 18/40\n",
      "1653/1653 [==============================] - 0s 208us/sample - loss: 1.6234 - acc: 0.8857 - val_loss: 1.8029 - val_acc: 0.8213\n",
      "Epoch 19/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.5807 - acc: 0.8857 - val_loss: 1.7717 - val_acc: 0.8357\n",
      "Epoch 20/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.5734 - acc: 0.8917 - val_loss: 1.7518 - val_acc: 0.8309\n",
      "Epoch 21/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.5303 - acc: 0.9020 - val_loss: 1.8050 - val_acc: 0.8357\n",
      "Epoch 22/40\n",
      "1653/1653 [==============================] - 0s 210us/sample - loss: 1.5457 - acc: 0.8851 - val_loss: 1.7727 - val_acc: 0.8309\n",
      "Epoch 23/40\n",
      "1653/1653 [==============================] - 0s 209us/sample - loss: 1.5009 - acc: 0.8941 - val_loss: 1.7495 - val_acc: 0.8357\n",
      "Epoch 24/40\n",
      "1653/1653 [==============================] - 0s 209us/sample - loss: 1.5218 - acc: 0.8923 - val_loss: 1.6988 - val_acc: 0.8309\n",
      "Epoch 25/40\n",
      "1653/1653 [==============================] - 0s 214us/sample - loss: 1.5335 - acc: 0.8724 - val_loss: 1.5978 - val_acc: 0.8261\n",
      "Epoch 26/40\n",
      "1653/1653 [==============================] - 0s 210us/sample - loss: 1.4722 - acc: 0.8947 - val_loss: 1.6174 - val_acc: 0.8406\n",
      "Epoch 27/40\n",
      "1653/1653 [==============================] - 0s 209us/sample - loss: 1.4646 - acc: 0.8905 - val_loss: 1.5555 - val_acc: 0.8454\n",
      "Epoch 28/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 1.4269 - acc: 0.8972 - val_loss: 1.5632 - val_acc: 0.8502\n",
      "Epoch 29/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 1.3898 - acc: 0.8996 - val_loss: 1.5463 - val_acc: 0.8261\n",
      "Epoch 30/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 1.3817 - acc: 0.8990 - val_loss: 1.5939 - val_acc: 0.8599\n",
      "Epoch 31/40\n",
      "1653/1653 [==============================] - 0s 209us/sample - loss: 1.3665 - acc: 0.9026 - val_loss: 1.6017 - val_acc: 0.8454\n",
      "Epoch 32/40\n",
      "1653/1653 [==============================] - 0s 216us/sample - loss: 1.3559 - acc: 0.9020 - val_loss: 1.5944 - val_acc: 0.8454\n",
      "Train: 0.929, Test: 0.845\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.90       167\n",
      "           1       0.70      0.18      0.29        39\n",
      "\n",
      "    accuracy                           0.83       206\n",
      "   macro avg       0.77      0.58      0.59       206\n",
      "weighted avg       0.81      0.83      0.79       206\n",
      "\n",
      "\n",
      "Fold  8\n",
      "Train on 1653 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1653/1653 [==============================] - 2s 1ms/sample - loss: 5.0990 - acc: 0.8088 - val_loss: 4.4587 - val_acc: 0.7971\n",
      "Epoch 2/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 4.1085 - acc: 0.8155 - val_loss: 3.6660 - val_acc: 0.7971\n",
      "Epoch 3/40\n",
      "1653/1653 [==============================] - 0s 214us/sample - loss: 3.4122 - acc: 0.8155 - val_loss: 3.1197 - val_acc: 0.7971\n",
      "Epoch 4/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 2.9205 - acc: 0.8185 - val_loss: 2.7625 - val_acc: 0.7971\n",
      "Epoch 5/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 2.5847 - acc: 0.8197 - val_loss: 2.4807 - val_acc: 0.8116\n",
      "Epoch 6/40\n",
      "1653/1653 [==============================] - 0s 214us/sample - loss: 2.3488 - acc: 0.8324 - val_loss: 2.3086 - val_acc: 0.8261\n",
      "Epoch 7/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 2.1909 - acc: 0.8421 - val_loss: 2.1859 - val_acc: 0.8164\n",
      "Epoch 8/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 2.0608 - acc: 0.8651 - val_loss: 2.1642 - val_acc: 0.8068\n",
      "Epoch 9/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.9768 - acc: 0.8669 - val_loss: 2.1052 - val_acc: 0.8164\n",
      "Epoch 10/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.9187 - acc: 0.8596 - val_loss: 1.9798 - val_acc: 0.8261\n",
      "Epoch 11/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 1.8558 - acc: 0.8663 - val_loss: 1.9372 - val_acc: 0.7923\n",
      "Epoch 12/40\n",
      "1653/1653 [==============================] - 0s 210us/sample - loss: 1.8354 - acc: 0.8772 - val_loss: 1.9039 - val_acc: 0.8116\n",
      "Epoch 13/40\n",
      "1653/1653 [==============================] - 0s 209us/sample - loss: 1.7784 - acc: 0.8730 - val_loss: 1.9473 - val_acc: 0.8261\n",
      "Epoch 14/40\n",
      "1653/1653 [==============================] - 0s 218us/sample - loss: 1.7494 - acc: 0.8808 - val_loss: 1.9293 - val_acc: 0.8164\n",
      "Epoch 15/40\n",
      "1653/1653 [==============================] - 0s 214us/sample - loss: 1.7140 - acc: 0.8754 - val_loss: 1.8654 - val_acc: 0.8116\n",
      "Epoch 16/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.6953 - acc: 0.8724 - val_loss: 1.8258 - val_acc: 0.8213\n",
      "Epoch 17/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 1.6527 - acc: 0.8869 - val_loss: 1.8187 - val_acc: 0.7874\n",
      "Epoch 18/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 1.6161 - acc: 0.8917 - val_loss: 1.8649 - val_acc: 0.8068\n",
      "Epoch 19/40\n",
      "1653/1653 [==============================] - 0s 210us/sample - loss: 1.6100 - acc: 0.8887 - val_loss: 1.8083 - val_acc: 0.8213\n",
      "Epoch 20/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.5848 - acc: 0.8832 - val_loss: 1.7241 - val_acc: 0.8068\n",
      "Epoch 21/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.5513 - acc: 0.8917 - val_loss: 1.7286 - val_acc: 0.8019\n",
      "Epoch 22/40\n",
      "1653/1653 [==============================] - 0s 208us/sample - loss: 1.5371 - acc: 0.8778 - val_loss: 1.7607 - val_acc: 0.8261\n",
      "Epoch 23/40\n",
      "1653/1653 [==============================] - 0s 214us/sample - loss: 1.5003 - acc: 0.8863 - val_loss: 1.7203 - val_acc: 0.8116\n",
      "Epoch 24/40\n",
      "1653/1653 [==============================] - 0s 210us/sample - loss: 1.4917 - acc: 0.8923 - val_loss: 1.6879 - val_acc: 0.8309\n",
      "Epoch 25/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.4636 - acc: 0.9014 - val_loss: 1.6396 - val_acc: 0.8116\n",
      "Epoch 26/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 1.4350 - acc: 0.9050 - val_loss: 1.6312 - val_acc: 0.7971\n",
      "Epoch 27/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 1.4246 - acc: 0.8863 - val_loss: 1.6261 - val_acc: 0.8309\n",
      "Epoch 28/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 1.4035 - acc: 0.9038 - val_loss: 1.6986 - val_acc: 0.8116\n",
      "Epoch 29/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.3904 - acc: 0.8947 - val_loss: 1.7331 - val_acc: 0.8164\n",
      "Epoch 30/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 1.3813 - acc: 0.8966 - val_loss: 1.6237 - val_acc: 0.8116\n",
      "Epoch 31/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 1.3837 - acc: 0.8935 - val_loss: 1.5884 - val_acc: 0.8116\n",
      "Epoch 32/40\n",
      "1653/1653 [==============================] - 0s 214us/sample - loss: 1.3356 - acc: 0.8887 - val_loss: 1.5301 - val_acc: 0.8068\n",
      "Epoch 33/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 1.3421 - acc: 0.8941 - val_loss: 1.5694 - val_acc: 0.8116\n",
      "Epoch 34/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 1.3202 - acc: 0.8917 - val_loss: 1.5901 - val_acc: 0.7923\n",
      "Epoch 35/40\n",
      "1653/1653 [==============================] - 0s 210us/sample - loss: 1.2888 - acc: 0.9026 - val_loss: 1.5683 - val_acc: 0.8261\n",
      "Train: 0.972, Test: 0.826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       165\n",
      "           1       0.67      0.34      0.45        41\n",
      "\n",
      "    accuracy                           0.83       206\n",
      "   macro avg       0.76      0.65      0.68       206\n",
      "weighted avg       0.82      0.83      0.81       206\n",
      "\n",
      "\n",
      "Fold  9\n",
      "Train on 1653 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1653/1653 [==============================] - 2s 1ms/sample - loss: 5.0995 - acc: 0.7054 - val_loss: 4.3785 - val_acc: 0.8019\n",
      "Epoch 2/40\n",
      "1653/1653 [==============================] - 0s 219us/sample - loss: 4.0221 - acc: 0.8052 - val_loss: 3.5264 - val_acc: 0.8019\n",
      "Epoch 3/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 3.3120 - acc: 0.8064 - val_loss: 2.9691 - val_acc: 0.8019\n",
      "Epoch 4/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 2.8364 - acc: 0.8064 - val_loss: 2.6005 - val_acc: 0.8019\n",
      "Epoch 5/40\n",
      "1653/1653 [==============================] - 0s 216us/sample - loss: 2.5604 - acc: 0.8070 - val_loss: 2.3624 - val_acc: 0.8068\n",
      "Epoch 6/40\n",
      "1653/1653 [==============================] - 0s 214us/sample - loss: 2.3196 - acc: 0.8076 - val_loss: 2.2144 - val_acc: 0.8068\n",
      "Epoch 7/40\n",
      "1653/1653 [==============================] - 0s 219us/sample - loss: 2.1599 - acc: 0.8119 - val_loss: 2.0879 - val_acc: 0.8068\n",
      "Epoch 8/40\n",
      "1653/1653 [==============================] - 0s 217us/sample - loss: 2.0399 - acc: 0.8191 - val_loss: 2.0043 - val_acc: 0.8213\n",
      "Epoch 9/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.9731 - acc: 0.8288 - val_loss: 1.9640 - val_acc: 0.8164\n",
      "Epoch 10/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 1.9069 - acc: 0.8397 - val_loss: 1.9292 - val_acc: 0.8116\n",
      "Epoch 11/40\n",
      "1653/1653 [==============================] - 0s 216us/sample - loss: 1.8834 - acc: 0.8409 - val_loss: 1.9237 - val_acc: 0.8213\n",
      "Epoch 12/40\n",
      "1653/1653 [==============================] - 0s 215us/sample - loss: 1.8366 - acc: 0.8439 - val_loss: 1.8688 - val_acc: 0.8116\n",
      "Epoch 13/40\n",
      "1653/1653 [==============================] - 0s 219us/sample - loss: 1.7993 - acc: 0.8524 - val_loss: 1.8911 - val_acc: 0.8213\n",
      "Epoch 14/40\n",
      "1653/1653 [==============================] - 0s 216us/sample - loss: 1.7683 - acc: 0.8609 - val_loss: 1.7971 - val_acc: 0.8164\n",
      "Epoch 15/40\n",
      "1653/1653 [==============================] - 0s 215us/sample - loss: 1.7430 - acc: 0.8596 - val_loss: 1.8021 - val_acc: 0.8019\n",
      "Epoch 16/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 1.7394 - acc: 0.8548 - val_loss: 1.7950 - val_acc: 0.8116\n",
      "Epoch 17/40\n",
      "1653/1653 [==============================] - 0s 215us/sample - loss: 1.6997 - acc: 0.8639 - val_loss: 1.7454 - val_acc: 0.8261\n",
      "Epoch 18/40\n",
      "1653/1653 [==============================] - 0s 219us/sample - loss: 1.6963 - acc: 0.8705 - val_loss: 1.7650 - val_acc: 0.7778\n",
      "Epoch 19/40\n",
      "1653/1653 [==============================] - 0s 215us/sample - loss: 1.6712 - acc: 0.8596 - val_loss: 1.7322 - val_acc: 0.7971\n",
      "Epoch 20/40\n",
      "1653/1653 [==============================] - 0s 216us/sample - loss: 1.6595 - acc: 0.8657 - val_loss: 1.7409 - val_acc: 0.8213\n",
      "Epoch 21/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.6196 - acc: 0.8590 - val_loss: 1.6896 - val_acc: 0.8213\n",
      "Epoch 22/40\n",
      "1653/1653 [==============================] - 0s 218us/sample - loss: 1.5744 - acc: 0.8857 - val_loss: 1.6702 - val_acc: 0.8116\n",
      "Epoch 23/40\n",
      "1653/1653 [==============================] - 0s 220us/sample - loss: 1.5631 - acc: 0.8881 - val_loss: 1.6800 - val_acc: 0.7971\n",
      "Epoch 24/40\n",
      "1653/1653 [==============================] - 0s 215us/sample - loss: 1.5171 - acc: 0.8947 - val_loss: 1.6285 - val_acc: 0.8213\n",
      "Epoch 25/40\n",
      "1653/1653 [==============================] - 0s 217us/sample - loss: 1.5174 - acc: 0.8826 - val_loss: 1.6179 - val_acc: 0.8164\n",
      "Epoch 26/40\n",
      "1653/1653 [==============================] - 0s 214us/sample - loss: 1.4869 - acc: 0.8857 - val_loss: 1.5860 - val_acc: 0.8261\n",
      "Epoch 27/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 1.4598 - acc: 0.9050 - val_loss: 1.6123 - val_acc: 0.8164\n",
      "Epoch 28/40\n",
      "1653/1653 [==============================] - 0s 216us/sample - loss: 1.4814 - acc: 0.8790 - val_loss: 1.6587 - val_acc: 0.8068\n",
      "Epoch 29/40\n",
      "1653/1653 [==============================] - 0s 215us/sample - loss: 1.4393 - acc: 0.8935 - val_loss: 1.7353 - val_acc: 0.8164\n",
      "Train: 0.929, Test: 0.816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94       180\n",
      "           1       0.60      0.35      0.44        26\n",
      "\n",
      "    accuracy                           0.89       206\n",
      "   macro avg       0.76      0.66      0.69       206\n",
      "weighted avg       0.87      0.89      0.88       206\n",
      "\n",
      "\n",
      "Fold  10\n",
      "Train on 1653 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1653/1653 [==============================] - 2s 1ms/sample - loss: 5.0746 - acc: 0.7810 - val_loss: 4.4271 - val_acc: 0.7826\n",
      "Epoch 2/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 4.0687 - acc: 0.8149 - val_loss: 3.6227 - val_acc: 0.7826\n",
      "Epoch 3/40\n",
      "1653/1653 [==============================] - 0s 210us/sample - loss: 3.3739 - acc: 0.8149 - val_loss: 3.1144 - val_acc: 0.7826\n",
      "Epoch 4/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 2.9012 - acc: 0.8149 - val_loss: 2.7505 - val_acc: 0.7826\n",
      "Epoch 5/40\n",
      "1653/1653 [==============================] - 0s 214us/sample - loss: 2.5562 - acc: 0.8179 - val_loss: 2.5085 - val_acc: 0.7826\n",
      "Epoch 6/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 2.3237 - acc: 0.8282 - val_loss: 2.3300 - val_acc: 0.8068\n",
      "Epoch 7/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 2.1723 - acc: 0.8415 - val_loss: 2.1888 - val_acc: 0.8213\n",
      "Epoch 8/40\n",
      "1653/1653 [==============================] - 0s 214us/sample - loss: 2.0623 - acc: 0.8500 - val_loss: 2.0910 - val_acc: 0.8454\n",
      "Epoch 9/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.9744 - acc: 0.8590 - val_loss: 2.0252 - val_acc: 0.8261\n",
      "Epoch 10/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 1.8987 - acc: 0.8693 - val_loss: 1.9751 - val_acc: 0.8357\n",
      "Epoch 11/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 1.8269 - acc: 0.8875 - val_loss: 1.9588 - val_acc: 0.8309\n",
      "Epoch 12/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 1.8073 - acc: 0.8772 - val_loss: 1.9279 - val_acc: 0.8213\n",
      "Epoch 13/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.7620 - acc: 0.8857 - val_loss: 1.9305 - val_acc: 0.8309\n",
      "Epoch 14/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.7375 - acc: 0.8929 - val_loss: 1.8612 - val_acc: 0.8213\n",
      "Epoch 15/40\n",
      "1653/1653 [==============================] - 0s 210us/sample - loss: 1.6996 - acc: 0.8923 - val_loss: 1.8690 - val_acc: 0.8116\n",
      "Epoch 16/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 1.6808 - acc: 0.8845 - val_loss: 1.8242 - val_acc: 0.7874\n",
      "Epoch 17/40\n",
      "1653/1653 [==============================] - 0s 216us/sample - loss: 1.6666 - acc: 0.8887 - val_loss: 1.7791 - val_acc: 0.8164\n",
      "Epoch 18/40\n",
      "1653/1653 [==============================] - 0s 217us/sample - loss: 1.6305 - acc: 0.8863 - val_loss: 1.7801 - val_acc: 0.8164\n",
      "Epoch 19/40\n",
      "1653/1653 [==============================] - 0s 209us/sample - loss: 1.6112 - acc: 0.8923 - val_loss: 1.8117 - val_acc: 0.7971\n",
      "Epoch 20/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 1.5828 - acc: 0.8953 - val_loss: 1.7441 - val_acc: 0.8164\n",
      "Epoch 21/40\n",
      "1653/1653 [==============================] - 0s 209us/sample - loss: 1.5461 - acc: 0.9068 - val_loss: 1.7746 - val_acc: 0.8164\n",
      "Epoch 22/40\n",
      "1653/1653 [==============================] - 0s 210us/sample - loss: 1.5222 - acc: 0.9087 - val_loss: 1.8774 - val_acc: 0.8164\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1653/1653 [==============================] - 0s 209us/sample - loss: 1.5411 - acc: 0.8953 - val_loss: 1.7269 - val_acc: 0.8068\n",
      "Epoch 24/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.5174 - acc: 0.8820 - val_loss: 1.7276 - val_acc: 0.7585\n",
      "Epoch 25/40\n",
      "1653/1653 [==============================] - 0s 214us/sample - loss: 1.4954 - acc: 0.9050 - val_loss: 1.7218 - val_acc: 0.7729\n",
      "Epoch 26/40\n",
      "1653/1653 [==============================] - 0s 211us/sample - loss: 1.4614 - acc: 0.9038 - val_loss: 1.6937 - val_acc: 0.8261\n",
      "Epoch 27/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 1.4253 - acc: 0.9165 - val_loss: 1.6463 - val_acc: 0.8309\n",
      "Epoch 28/40\n",
      "1653/1653 [==============================] - 0s 212us/sample - loss: 1.4120 - acc: 0.9087 - val_loss: 1.6185 - val_acc: 0.8019\n",
      "Epoch 29/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 1.3912 - acc: 0.9177 - val_loss: 1.6133 - val_acc: 0.7971\n",
      "Epoch 30/40\n",
      "1653/1653 [==============================] - 0s 209us/sample - loss: 1.3659 - acc: 0.9123 - val_loss: 1.5955 - val_acc: 0.8309\n",
      "Epoch 31/40\n",
      "1653/1653 [==============================] - 0s 209us/sample - loss: 1.3536 - acc: 0.9087 - val_loss: 1.6328 - val_acc: 0.8261\n",
      "Epoch 32/40\n",
      "1653/1653 [==============================] - 0s 213us/sample - loss: 1.3169 - acc: 0.9256 - val_loss: 1.6146 - val_acc: 0.8213\n",
      "Epoch 33/40\n",
      "1653/1653 [==============================] - 0s 215us/sample - loss: 1.3014 - acc: 0.9208 - val_loss: 1.6254 - val_acc: 0.8261\n",
      "Train: 0.966, Test: 0.826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.92       169\n",
      "           1       0.79      0.30      0.43        37\n",
      "\n",
      "    accuracy                           0.86       206\n",
      "   macro avg       0.83      0.64      0.68       206\n",
      "weighted avg       0.85      0.86      0.83       206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "num_epochs = 40\n",
    "\n",
    "for train_index, test_index in kf.split(word_seq_train):\n",
    "    x_trn, x_tst = word_seq_train[train_index], word_seq_train[test_index]\n",
    "    y_trn, y_tst = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    x_new_train, x_val, y_new_train, y_val= train_test_split(x_trn, y_trn, test_size=0.11115, random_state=125)\n",
    "    \n",
    "    print(\"\\nFold \", count)\n",
    "    lstm_model=LSTM_model()\n",
    "\n",
    "    \n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "    \n",
    "    history =lstm_model.fit( x_new_train, y_new_train, batch_size=256,\n",
    "          epochs=num_epochs, validation_data=(x_val, y_val), callbacks=[es_callback], shuffle=False)\n",
    "    \n",
    "    _, train_acc =lstm_model.evaluate(x_new_train,  y_new_train, verbose=0)\n",
    "    _, val_acc =  lstm_model.evaluate(x_val, y_val, verbose=0)\n",
    "    print('Train: %.3f, Test: %.3f' % (train_acc, val_acc))\n",
    "    \n",
    "    \n",
    "     \n",
    "    #plt.savefig('LSTM with fasttext SE data accuracy graph.png')\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "    y_pred = lstm_model.predict(x_tst)\n",
    "    y_pred = (y_pred >= 0.5)\n",
    " \n",
    "    \n",
    "    from sklearn import metrics\n",
    "    print(metrics.classification_report(y_tst, y_pred))\n",
    "    \n",
    "    lstm_precision = precision_score(y_tst, y_pred, pos_label=1)\n",
    "    lstm_recall = recall_score(y_tst, y_pred, pos_label=1)\n",
    "    lstm_f1score = f1_score(y_tst, y_pred, pos_label=1)\n",
    "    lstm_accuracy = accuracy_score(y_tst, y_pred)\n",
    "\n",
    "    lstm_run_accuracy.append(lstm_accuracy)\n",
    "    lstm_run_f1score.append(lstm_f1score)\n",
    "    lstm_run_precision.append(lstm_precision)\n",
    "    lstm_run_recall.append(lstm_recall)\n",
    "    \n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e818d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for bilstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c021700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM,GlobalMaxPool1D\n",
    "maxlen=max_seq_len\n",
    "embed_size=300\n",
    "max_features=nb_words\n",
    "def Bi_LSTM_base():\n",
    "    inp = keras.layers.Input(shape=(maxlen,))\n",
    "    x = tensorflow.keras.layers.Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = tensorflow.keras.layers.Bidirectional(tensorflow.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "    x = tensorflow.keras.layers.GlobalMaxPool1D()(x)\n",
    "    x = tensorflow.keras.layers.Dense(50, activation=\"relu\")(x)\n",
    "    x = tensorflow.keras.layers.Dropout(0.1)(x)\n",
    "    x = tensorflow.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "05cc99f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 57)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_18 (Embedding)     (None, 57, 300)           2173800   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 57, 100)           140400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 2,319,301\n",
      "Trainable params: 2,319,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Bi_LSTM_base().summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "028a05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blbase_run_precision = []\n",
    "blbase_run_recall = []\n",
    "blbase_run_f1score = []\n",
    "blbase_run_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8d40cddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  1\n",
      "Train on 1652 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1652/1652 [==============================] - 8s 5ms/sample - loss: 0.5059 - acc: 0.8063 - val_loss: 0.4537 - val_acc: 0.8019\n",
      "Epoch 2/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.3583 - acc: 0.8311 - val_loss: 0.3455 - val_acc: 0.8599\n",
      "Epoch 3/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.1632 - acc: 0.9401 - val_loss: 0.3852 - val_acc: 0.8502\n",
      "Epoch 4/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0393 - acc: 0.9897 - val_loss: 0.6935 - val_acc: 0.8647\n",
      "Epoch 5/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0114 - acc: 0.9988 - val_loss: 0.7933 - val_acc: 0.8551\n",
      "Train: 0.999, Test: 0.855\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91       165\n",
      "           1       0.73      0.38      0.50        42\n",
      "\n",
      "    accuracy                           0.85       207\n",
      "   macro avg       0.79      0.67      0.70       207\n",
      "weighted avg       0.83      0.85      0.83       207\n",
      "\n",
      "\n",
      "Fold  2\n",
      "Train on 1652 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1652/1652 [==============================] - 8s 5ms/sample - loss: 0.5032 - acc: 0.8063 - val_loss: 0.4372 - val_acc: 0.8164\n",
      "Epoch 2/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.3695 - acc: 0.8154 - val_loss: 0.3385 - val_acc: 0.8309\n",
      "Epoch 3/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.1826 - acc: 0.9334 - val_loss: 0.3193 - val_acc: 0.8696\n",
      "Epoch 4/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0579 - acc: 0.9824 - val_loss: 0.4211 - val_acc: 0.8599\n",
      "Epoch 5/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0320 - acc: 0.9933 - val_loss: 0.4058 - val_acc: 0.8551\n",
      "Epoch 6/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0222 - acc: 0.9939 - val_loss: 0.4612 - val_acc: 0.8357\n",
      "Train: 0.998, Test: 0.836\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       172\n",
      "           1       0.61      0.71      0.66        35\n",
      "\n",
      "    accuracy                           0.87       207\n",
      "   macro avg       0.77      0.81      0.79       207\n",
      "weighted avg       0.88      0.87      0.88       207\n",
      "\n",
      "\n",
      "Fold  3\n",
      "Train on 1652 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1652/1652 [==============================] - 8s 5ms/sample - loss: 0.4946 - acc: 0.8087 - val_loss: 0.4018 - val_acc: 0.8261\n",
      "Epoch 2/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.3426 - acc: 0.8366 - val_loss: 0.3159 - val_acc: 0.8551\n",
      "Epoch 3/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.1706 - acc: 0.9443 - val_loss: 0.3372 - val_acc: 0.8744\n",
      "Epoch 4/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0671 - acc: 0.9788 - val_loss: 0.3795 - val_acc: 0.8792\n",
      "Epoch 5/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0420 - acc: 0.9879 - val_loss: 0.2796 - val_acc: 0.8744\n",
      "Epoch 6/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0185 - acc: 0.9976 - val_loss: 0.3762 - val_acc: 0.8937\n",
      "Epoch 7/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0068 - acc: 0.9988 - val_loss: 0.4019 - val_acc: 0.8744\n",
      "Epoch 8/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0076 - acc: 0.9988 - val_loss: 0.4211 - val_acc: 0.8744\n",
      "Train: 0.999, Test: 0.874\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       171\n",
      "           1       0.62      0.67      0.64        36\n",
      "\n",
      "    accuracy                           0.87       207\n",
      "   macro avg       0.77      0.79      0.78       207\n",
      "weighted avg       0.87      0.87      0.87       207\n",
      "\n",
      "\n",
      "Fold  4\n",
      "Train on 1652 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1652/1652 [==============================] - 8s 5ms/sample - loss: 0.5023 - acc: 0.8021 - val_loss: 0.4862 - val_acc: 0.7826\n",
      "Epoch 2/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.3487 - acc: 0.8299 - val_loss: 0.3829 - val_acc: 0.8261\n",
      "Epoch 3/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.1774 - acc: 0.9340 - val_loss: 0.4401 - val_acc: 0.8454\n",
      "Epoch 4/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0627 - acc: 0.9770 - val_loss: 0.5264 - val_acc: 0.7923\n",
      "Epoch 5/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0387 - acc: 0.9885 - val_loss: 0.5744 - val_acc: 0.7971\n",
      "Train: 0.998, Test: 0.797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88       158\n",
      "           1       0.61      0.57      0.59        49\n",
      "\n",
      "    accuracy                           0.81       207\n",
      "   macro avg       0.74      0.73      0.73       207\n",
      "weighted avg       0.81      0.81      0.81       207\n",
      "\n",
      "\n",
      "Fold  5\n",
      "Train on 1652 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1652/1652 [==============================] - 8s 5ms/sample - loss: 0.5101 - acc: 0.8033 - val_loss: 0.4794 - val_acc: 0.7971\n",
      "Epoch 2/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.3764 - acc: 0.8184 - val_loss: 0.3777 - val_acc: 0.8116\n",
      "Epoch 3/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.2016 - acc: 0.9183 - val_loss: 0.4431 - val_acc: 0.8406\n",
      "Epoch 4/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0676 - acc: 0.9812 - val_loss: 0.5777 - val_acc: 0.8454\n",
      "Epoch 5/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0311 - acc: 0.9903 - val_loss: 0.5396 - val_acc: 0.8454\n",
      "Train: 0.994, Test: 0.845\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.84      0.87       164\n",
      "           1       0.53      0.70      0.60        43\n",
      "\n",
      "    accuracy                           0.81       207\n",
      "   macro avg       0.72      0.77      0.74       207\n",
      "weighted avg       0.83      0.81      0.82       207\n",
      "\n",
      "\n",
      "Fold  6\n",
      "Train on 1652 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1652/1652 [==============================] - 8s 5ms/sample - loss: 0.5041 - acc: 0.8105 - val_loss: 0.4641 - val_acc: 0.8019\n",
      "Epoch 2/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.3750 - acc: 0.8166 - val_loss: 0.3531 - val_acc: 0.8406\n",
      "Epoch 3/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.1932 - acc: 0.9268 - val_loss: 0.3640 - val_acc: 0.8696\n",
      "Epoch 4/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0588 - acc: 0.9837 - val_loss: 0.4824 - val_acc: 0.8599\n",
      "Epoch 5/40\n",
      "1652/1652 [==============================] - 5s 3ms/sample - loss: 0.0185 - acc: 0.9958 - val_loss: 0.6411 - val_acc: 0.8551\n",
      "Train: 0.999, Test: 0.855\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91       167\n",
      "           1       0.67      0.50      0.57        40\n",
      "\n",
      "    accuracy                           0.86       207\n",
      "   macro avg       0.78      0.72      0.74       207\n",
      "weighted avg       0.84      0.86      0.85       207\n",
      "\n",
      "\n",
      "Fold  7\n",
      "Train on 1653 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1653/1653 [==============================] - 8s 5ms/sample - loss: 0.5088 - acc: 0.8070 - val_loss: 0.4349 - val_acc: 0.8261\n",
      "Epoch 2/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.3653 - acc: 0.8221 - val_loss: 0.3295 - val_acc: 0.8599\n",
      "Epoch 3/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.1721 - acc: 0.9383 - val_loss: 0.3500 - val_acc: 0.8647\n",
      "Epoch 4/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.0491 - acc: 0.9837 - val_loss: 0.5044 - val_acc: 0.8599\n",
      "Epoch 5/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.0255 - acc: 0.9933 - val_loss: 0.5950 - val_acc: 0.8454\n",
      "Train: 0.999, Test: 0.845\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       167\n",
      "           1       0.72      0.46      0.56        39\n",
      "\n",
      "    accuracy                           0.86       206\n",
      "   macro avg       0.80      0.71      0.74       206\n",
      "weighted avg       0.85      0.86      0.85       206\n",
      "\n",
      "\n",
      "Fold  8\n",
      "Train on 1653 samples, validate on 207 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1653/1653 [==============================] - 8s 5ms/sample - loss: 0.4898 - acc: 0.8155 - val_loss: 0.4380 - val_acc: 0.7971\n",
      "Epoch 2/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.3423 - acc: 0.8391 - val_loss: 0.3562 - val_acc: 0.8454\n",
      "Epoch 3/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.1563 - acc: 0.9480 - val_loss: 0.4056 - val_acc: 0.8454\n",
      "Epoch 4/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.0593 - acc: 0.9849 - val_loss: 0.6372 - val_acc: 0.8599\n",
      "Epoch 5/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.0183 - acc: 0.9964 - val_loss: 0.6537 - val_acc: 0.8406\n",
      "Train: 1.000, Test: 0.841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88       165\n",
      "           1       0.52      0.37      0.43        41\n",
      "\n",
      "    accuracy                           0.81       206\n",
      "   macro avg       0.69      0.64      0.66       206\n",
      "weighted avg       0.79      0.81      0.79       206\n",
      "\n",
      "\n",
      "Fold  9\n",
      "Train on 1653 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1653/1653 [==============================] - 8s 5ms/sample - loss: 0.5136 - acc: 0.7846 - val_loss: 0.4621 - val_acc: 0.8019\n",
      "Epoch 2/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.3618 - acc: 0.8355 - val_loss: 0.4463 - val_acc: 0.8406\n",
      "Epoch 3/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.1686 - acc: 0.9401 - val_loss: 0.4995 - val_acc: 0.8454\n",
      "Epoch 4/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.0547 - acc: 0.9837 - val_loss: 0.4721 - val_acc: 0.8454\n",
      "Epoch 5/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.0170 - acc: 0.9982 - val_loss: 0.6192 - val_acc: 0.8164\n",
      "Train: 1.000, Test: 0.816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93       180\n",
      "           1       0.54      0.50      0.52        26\n",
      "\n",
      "    accuracy                           0.88       206\n",
      "   macro avg       0.74      0.72      0.73       206\n",
      "weighted avg       0.88      0.88      0.88       206\n",
      "\n",
      "\n",
      "Fold  10\n",
      "Train on 1653 samples, validate on 207 samples\n",
      "Epoch 1/40\n",
      "1653/1653 [==============================] - 9s 5ms/sample - loss: 0.5068 - acc: 0.8088 - val_loss: 0.4907 - val_acc: 0.7826\n",
      "Epoch 2/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.3792 - acc: 0.8173 - val_loss: 0.3858 - val_acc: 0.8213\n",
      "Epoch 3/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.2006 - acc: 0.9244 - val_loss: 0.4564 - val_acc: 0.8502\n",
      "Epoch 4/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.0545 - acc: 0.9825 - val_loss: 0.5791 - val_acc: 0.8502\n",
      "Epoch 5/40\n",
      "1653/1653 [==============================] - 5s 3ms/sample - loss: 0.0157 - acc: 0.9976 - val_loss: 0.6047 - val_acc: 0.8357\n",
      "Train: 0.996, Test: 0.836\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       169\n",
      "           1       0.62      0.62      0.62        37\n",
      "\n",
      "    accuracy                           0.86       206\n",
      "   macro avg       0.77      0.77      0.77       206\n",
      "weighted avg       0.86      0.86      0.86       206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "\n",
    "for train_index, test_index in kf.split(word_seq_train):\n",
    "    x_trn, x_tst = word_seq_train[train_index], word_seq_train[test_index]\n",
    "    y_trn, y_tst = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    x_new_train, x_val, y_new_train, y_val= train_test_split(x_trn, y_trn, test_size=0.11115, random_state=125)\n",
    "    \n",
    "    print(\"\\nFold \", count)\n",
    "    bilstmbase_model=Bi_LSTM_base()\n",
    "\n",
    "    #model_lstm_fasttext=model_with_embedding()\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "    \n",
    "    history =bilstmbase_model.fit( x_new_train, y_new_train, batch_size=32,\n",
    "          epochs=num_epochs, validation_data=(x_val, y_val), callbacks=[es_callback], shuffle=False)\n",
    "    \n",
    "    _, train_acc = bilstmbase_model.evaluate(x_new_train,  y_new_train, verbose=0)\n",
    "    _, val_acc =  bilstmbase_model.evaluate(x_val, y_val, verbose=0)\n",
    "    print('Train: %.3f, Test: %.3f' % (train_acc, val_acc))\n",
    "    \n",
    "    \n",
    "    y_pred = bilstmbase_model.predict(x_tst)\n",
    "    y_pred = (y_pred >= 0.5)\n",
    " \n",
    "    \n",
    "    from sklearn import metrics\n",
    "    print(metrics.classification_report(y_tst, y_pred))\n",
    "    \n",
    "    blbase_precision = precision_score(y_tst, y_pred, pos_label=1)\n",
    "    blbase_recall = recall_score(y_tst, y_pred, pos_label=1)\n",
    "    blbase_f1score = f1_score(y_tst, y_pred, pos_label=1)\n",
    "    blbase_accuracy = accuracy_score(y_tst, y_pred)\n",
    "\n",
    "    blbase_run_accuracy.append(blbase_accuracy)\n",
    "    blbase_run_f1score.append(blbase_f1score)\n",
    "    blbase_run_precision.append(blbase_precision)\n",
    "    blbase_run_recall.append(blbase_recall)\n",
    "    \n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa20e38e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
