{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "controlled-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "#print(tf.__version__)\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optmizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "toxic-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "####https://www.tensorflow.org/tutorials/text/classify_text_with_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "under-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=125)\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "improved-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import  precision_score\n",
    "from sklearn.metrics import  f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "artificial-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aboriginal-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_run_precision = []\n",
    "bert_run_recall = []\n",
    "bert_run_f1score = []\n",
    "bert_run_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "crude-rates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'bert_en_cased_L-12_H-768_A-12' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle['small_bert/bert_en_uncased_L-2_H-768_A-12']\n",
    "tfhub_handle_preprocess = map_model_to_preprocess['small_bert/bert_en_uncased_L-2_H-768_A-12']\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "crude-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-ordering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "packed-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "isolated-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='comment')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adverse-chapel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "comment (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "preprocessing (KerasLayer)      {'input_word_ids': ( 0           comment[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "BERT_encoder (KerasLayer)       {'encoder_outputs':  38603521    preprocessing[0][0]              \n",
      "                                                                 preprocessing[0][1]              \n",
      "                                                                 preprocessing[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           BERT_encoder[0][3]               \n",
      "__________________________________________________________________________________________________\n",
      "classifier (Dense)              (None, 1)            769         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 38,604,290\n",
      "Trainable params: 38,604,289\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model_test = build_classifier_model()\n",
    "\n",
    "classifier_model_test.summary()\n",
    "#bert_raw_result = classifier_model_test(tf.constant(text_test))\n",
    "#print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "qualified-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_excel('politeness_2k_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "approved-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(value='', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6af772a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['final_agreed_rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "smart-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_train=data.fillna(\"fillna\").values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "hired-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('target')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "activated-sitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold:  1\n",
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 124s 2s/step - loss: 0.9191 - binary_accuracy: 0.5077 - val_loss: 0.7700 - val_binary_accuracy: 0.6522\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 122s 2s/step - loss: 0.3979 - binary_accuracy: 0.8403 - val_loss: 0.4881 - val_binary_accuracy: 0.7778\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 123s 2s/step - loss: 0.2856 - binary_accuracy: 0.8788 - val_loss: 0.5041 - val_binary_accuracy: 0.7923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94       161\n",
      "           1       0.94      0.63      0.75        46\n",
      "\n",
      "    accuracy                           0.91       207\n",
      "   macro avg       0.92      0.81      0.85       207\n",
      "weighted avg       0.91      0.91      0.90       207\n",
      "\n",
      "Starting Fold:  2\n",
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 125s 2s/step - loss: 1.1714 - binary_accuracy: 0.4418 - val_loss: 0.6957 - val_binary_accuracy: 0.6377\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 122s 2s/step - loss: 0.3729 - binary_accuracy: 0.8431 - val_loss: 0.5529 - val_binary_accuracy: 0.7053\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 122s 2s/step - loss: 0.2502 - binary_accuracy: 0.8901 - val_loss: 0.5641 - val_binary_accuracy: 0.7343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93       161\n",
      "           1       0.82      0.61      0.70        46\n",
      "\n",
      "    accuracy                           0.88       207\n",
      "   macro avg       0.86      0.79      0.81       207\n",
      "weighted avg       0.88      0.88      0.88       207\n",
      "\n",
      "Starting Fold:  3\n",
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 124s 2s/step - loss: 0.5835 - binary_accuracy: 0.7920 - val_loss: 1.0146 - val_binary_accuracy: 0.4879\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 123s 2s/step - loss: 0.3455 - binary_accuracy: 0.8631 - val_loss: 0.8690 - val_binary_accuracy: 0.5749\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 123s 2s/step - loss: 0.2497 - binary_accuracy: 0.8914 - val_loss: 0.8223 - val_binary_accuracy: 0.6232\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 122s 2s/step - loss: 0.1812 - binary_accuracy: 0.9184 - val_loss: 0.7790 - val_binary_accuracy: 0.6667\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 122s 2s/step - loss: 0.1269 - binary_accuracy: 0.9469 - val_loss: 0.8640 - val_binary_accuracy: 0.6667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       166\n",
      "           1       0.85      0.80      0.83        41\n",
      "\n",
      "    accuracy                           0.93       207\n",
      "   macro avg       0.90      0.88      0.89       207\n",
      "weighted avg       0.93      0.93      0.93       207\n",
      "\n",
      "Starting Fold:  4\n",
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 121s 2s/step - loss: 0.5941 - binary_accuracy: 0.7911 - val_loss: 1.2811 - val_binary_accuracy: 0.3478\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 115s 2s/step - loss: 0.3176 - binary_accuracy: 0.8721 - val_loss: 1.3905 - val_binary_accuracy: 0.3768\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92       172\n",
      "           1       0.78      0.20      0.32        35\n",
      "\n",
      "    accuracy                           0.86       207\n",
      "   macro avg       0.82      0.59      0.62       207\n",
      "weighted avg       0.84      0.86      0.82       207\n",
      "\n",
      "Starting Fold:  5\n",
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 117s 2s/step - loss: 0.6789 - binary_accuracy: 0.7156 - val_loss: 0.1124 - val_binary_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 116s 2s/step - loss: 0.4404 - binary_accuracy: 0.8137 - val_loss: 0.1202 - val_binary_accuracy: 0.9710\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89       156\n",
      "           1       0.86      0.35      0.50        51\n",
      "\n",
      "    accuracy                           0.83       207\n",
      "   macro avg       0.84      0.67      0.70       207\n",
      "weighted avg       0.83      0.83      0.80       207\n",
      "\n",
      "Starting Fold:  6\n",
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 126s 2s/step - loss: 0.5248 - binary_accuracy: 0.7903 - val_loss: 0.3106 - val_binary_accuracy: 0.9855\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 122s 2s/step - loss: 0.4188 - binary_accuracy: 0.8166 - val_loss: 0.2748 - val_binary_accuracy: 0.9469\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 122s 2s/step - loss: 0.3014 - binary_accuracy: 0.8692 - val_loss: 0.2179 - val_binary_accuracy: 0.9517\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 122s 2s/step - loss: 0.2152 - binary_accuracy: 0.9043 - val_loss: 0.2115 - val_binary_accuracy: 0.9469\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 124s 2s/step - loss: 0.1689 - binary_accuracy: 0.9244 - val_loss: 0.2724 - val_binary_accuracy: 0.9130\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       156\n",
      "           1       0.75      0.76      0.76        51\n",
      "\n",
      "    accuracy                           0.88       207\n",
      "   macro avg       0.84      0.84      0.84       207\n",
      "weighted avg       0.88      0.88      0.88       207\n",
      "\n",
      "Starting Fold:  7\n",
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 124s 2s/step - loss: 0.5159 - binary_accuracy: 0.8001 - val_loss: 0.2092 - val_binary_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 122s 2s/step - loss: 0.3985 - binary_accuracy: 0.8214 - val_loss: 0.1076 - val_binary_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 122s 2s/step - loss: 0.2683 - binary_accuracy: 0.8738 - val_loss: 0.0732 - val_binary_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 122s 2s/step - loss: 0.1903 - binary_accuracy: 0.9193 - val_loss: 0.0419 - val_binary_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 122s 2s/step - loss: 0.1403 - binary_accuracy: 0.9395 - val_loss: 0.0295 - val_binary_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "52/52 [==============================] - 119s 2s/step - loss: 0.1207 - binary_accuracy: 0.9456 - val_loss: 0.0477 - val_binary_accuracy: 0.9951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91       156\n",
      "           1       0.76      0.69      0.72        51\n",
      "\n",
      "    accuracy                           0.87       207\n",
      "   macro avg       0.83      0.81      0.82       207\n",
      "weighted avg       0.87      0.87      0.87       207\n",
      "\n",
      "Starting Fold:  8\n",
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 125s 2s/step - loss: 0.5253 - binary_accuracy: 0.7995 - val_loss: 0.2709 - val_binary_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 122s 2s/step - loss: 0.4112 - binary_accuracy: 0.8080 - val_loss: 0.1986 - val_binary_accuracy: 0.9515\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 122s 2s/step - loss: 0.3003 - binary_accuracy: 0.8552 - val_loss: 0.2285 - val_binary_accuracy: 0.9272\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       156\n",
      "           1       0.74      0.73      0.73        51\n",
      "\n",
      "    accuracy                           0.87       207\n",
      "   macro avg       0.83      0.82      0.82       207\n",
      "weighted avg       0.87      0.87      0.87       207\n",
      "\n",
      "Starting Fold:  9\n",
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 117s 2s/step - loss: 0.5490 - binary_accuracy: 0.7950 - val_loss: 0.2560 - val_binary_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 114s 2s/step - loss: 0.4085 - binary_accuracy: 0.8132 - val_loss: 0.1324 - val_binary_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 114s 2s/step - loss: 0.2860 - binary_accuracy: 0.8788 - val_loss: 0.0834 - val_binary_accuracy: 0.9951\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 115s 2s/step - loss: 0.2119 - binary_accuracy: 0.9015 - val_loss: 0.0857 - val_binary_accuracy: 0.9806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91       156\n",
      "           1       0.72      0.67      0.69        51\n",
      "\n",
      "    accuracy                           0.86       207\n",
      "   macro avg       0.81      0.79      0.80       207\n",
      "weighted avg       0.85      0.86      0.85       207\n",
      "\n",
      "Starting Fold:  10\n",
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 116s 2s/step - loss: 0.5865 - binary_accuracy: 0.7551 - val_loss: 0.1934 - val_binary_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 114s 2s/step - loss: 0.4576 - binary_accuracy: 0.7874 - val_loss: 0.1081 - val_binary_accuracy: 0.9854\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 114s 2s/step - loss: 0.3056 - binary_accuracy: 0.8532 - val_loss: 0.0827 - val_binary_accuracy: 0.9854\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 114s 2s/step - loss: 0.2185 - binary_accuracy: 0.9065 - val_loss: 0.0520 - val_binary_accuracy: 0.9854\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 114s 2s/step - loss: 0.1699 - binary_accuracy: 0.9264 - val_loss: 0.0317 - val_binary_accuracy: 0.9903\n",
      "Epoch 6/10\n",
      "52/52 [==============================] - 114s 2s/step - loss: 0.1381 - binary_accuracy: 0.9355 - val_loss: 0.0208 - val_binary_accuracy: 0.9951\n",
      "Epoch 7/10\n",
      "52/52 [==============================] - 114s 2s/step - loss: 0.1063 - binary_accuracy: 0.9546 - val_loss: 0.0316 - val_binary_accuracy: 0.9854\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91       156\n",
      "           1       0.77      0.59      0.67        51\n",
      "\n",
      "    accuracy                           0.86       207\n",
      "   macro avg       0.82      0.77      0.79       207\n",
      "weighted avg       0.85      0.86      0.85       207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "fold_no = 1\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "for train_index, test_index in KFold(10).split(initial_train):\n",
    "    print(\"Starting Fold: \", fold_no)\n",
    "    \n",
    "    x_train, x_val = initial_train[train_index], initial_train[test_index]\n",
    "    \n",
    "    train = pd.DataFrame(data=x_train, index=[i for i in range(x_train.shape[0])], columns=[ \"comment\", \"target\"])\n",
    "     \n",
    "    val = pd.DataFrame(data=x_val, index=[i for i in range(x_val.shape[0])], columns=[\"comment\", \"target\"])\n",
    "    \n",
    "    x_train, x_test= train_test_split(x_train,  test_size=0.11115, random_state=125)\n",
    "\n",
    "    train = pd.DataFrame(data=x_train, index=[i for i in range(x_train.shape[0])], columns=[\"comment\", \"target\"])\n",
    "    test = pd.DataFrame(data=x_test, index=[i for i in range(x_test.shape[0])], columns=[\"comment\", \"target\"])\n",
    "    \n",
    "    #print(train)\n",
    "    \n",
    "    #ptrain=train\n",
    "    \n",
    "    data_batch_size=32\n",
    "    train.fillna(value='', inplace=True)\n",
    "    \n",
    "    train['target'] = np.where(train['target']== 0,0,1)\n",
    "     \n",
    "    ptrain_df = df_to_dataset(train, batch_size=data_batch_size)\n",
    "    \n",
    "    \n",
    "    test.fillna(value='', inplace=True)\n",
    "    test['target'] = np.where(test['target']== 0,0,1)\n",
    "     \n",
    "    ptest_df = df_to_dataset(test, batch_size=data_batch_size)\n",
    "    \n",
    "    \n",
    "    #test['target'] = np.where(test['is_toxic']== 0,0,1)\n",
    "     \n",
    "    #test_df = df_to_dataset(test, batch_size=data_batch_size)\n",
    "    \n",
    "    val.fillna(value='', inplace=True)\n",
    "    val['target'] = np.where(val['target']== 0,0,1)\n",
    "     \n",
    "    pval_df = df_to_dataset(val, batch_size=data_batch_size)\n",
    "    \n",
    "    \n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_dp = ptrain_df.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_dp = pval_df.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test_dp = ptest_df.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    ##load model\n",
    "    classifier_model= build_classifier_model()\n",
    "\n",
    "    \n",
    "    ###start train\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = tf.metrics.BinaryAccuracy()\n",
    "    \n",
    "    \n",
    "    epochs = 10\n",
    "    steps_per_epoch = tf.data.experimental.cardinality(train_dp).numpy()\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "    init_lr = 3e-5\n",
    "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')\n",
    "    \n",
    "    classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    \n",
    "    ##set early stopping \n",
    "    callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
    "    \n",
    "    ##training started\n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    history = classifier_model.fit(x=train_dp,\n",
    "                               validation_data=val_dp,\n",
    "                               epochs=epochs,  callbacks=[callbacks])\n",
    "    \n",
    "    ##testing_model\n",
    "    y_tst=test['target'].values\n",
    "    y_pred = tf.sigmoid(classifier_model(tf.constant(test[\"comment\"])))\n",
    "    y_pred = (y_pred >= 0.5)\n",
    "    \n",
    "    \n",
    "    #print(y_tst, y_pred)\n",
    "    ##classification metrics\n",
    "    from sklearn import metrics\n",
    "    print(metrics.classification_report(y_tst, y_pred))\n",
    "    \n",
    "    bert_precision = precision_score(y_tst, y_pred, pos_label=1)\n",
    "    bert_recall = recall_score(y_tst, y_pred, pos_label=1)\n",
    "    bert_f1score = f1_score(y_tst, y_pred, pos_label=1)\n",
    "    bert_accuracy = accuracy_score(y_tst, y_pred)\n",
    "\n",
    "    bert_run_accuracy.append(bert_accuracy)\n",
    "    bert_run_f1score.append(bert_f1score)\n",
    "    bert_run_precision.append(bert_precision)\n",
    "    bert_run_recall.append(bert_recall)\n",
    "    \n",
    "    fold_no = fold_no+1\n",
    "\n",
    "    \n",
    "    #print(len(train))\n",
    "    #print(len(val))\n",
    "    #print(len(test))\n",
    "    #indexs=[train_index]\n",
    "    #val_indexes=[test_index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-crystal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
