{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlled-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "#print(tf.__version__)\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optmizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6fa3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "toxic-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "####https://www.tensorflow.org/tutorials/text/classify_text_with_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "under-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=125)\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "improved-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import  precision_score\n",
    "from sklearn.metrics import  f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "artificial-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aboriginal-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_run_precision = []\n",
    "bert_run_recall = []\n",
    "bert_run_f1score = []\n",
    "bert_run_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crude-rates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'bert_en_cased_L-12_H-768_A-12' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "crude-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-ordering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "packed-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "isolated-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='comment')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adverse-chapel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "comment (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "preprocessing (KerasLayer)      {'input_word_ids': ( 0           comment[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "BERT_encoder (KerasLayer)       {'sequence_output':  108310273   preprocessing[0][0]              \n",
      "                                                                 preprocessing[0][1]              \n",
      "                                                                 preprocessing[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 768)          0           BERT_encoder[0][13]              \n",
      "__________________________________________________________________________________________________\n",
      "classifier (Dense)              (None, 1)            769         dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 108,311,042\n",
      "Trainable params: 108,311,041\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model_test = build_classifier_model()\n",
    "\n",
    "classifier_model_test.summary()\n",
    "#bert_raw_result = classifier_model_test(tf.constant(text_test))\n",
    "#print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "qualified-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_excel('politeness_2k_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "approved-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(value='', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6af772a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['final_agreed_rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7af526a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['stanford_tool_score'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "smart-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_train=data.fillna(\"fillna\").values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "hired-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('target')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "activated-sitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold:  1\n",
      "Training model with https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 18s 339ms/step - loss: 0.5793 - binary_accuracy: 0.7785 - val_loss: 0.6626 - val_binary_accuracy: 0.6522\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 17s 329ms/step - loss: 0.3378 - binary_accuracy: 0.8656 - val_loss: 0.4485 - val_binary_accuracy: 0.8019\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 17s 334ms/step - loss: 0.1725 - binary_accuracy: 0.9346 - val_loss: 0.4544 - val_binary_accuracy: 0.8068\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95       161\n",
      "           1       0.88      0.78      0.83        46\n",
      "\n",
      "    accuracy                           0.93       207\n",
      "   macro avg       0.91      0.88      0.89       207\n",
      "weighted avg       0.93      0.93      0.93       207\n",
      "\n",
      "Starting Fold:  2\n",
      "Training model with https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 18s 349ms/step - loss: 0.4745 - binary_accuracy: 0.8360 - val_loss: 0.6991 - val_binary_accuracy: 0.6377\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 17s 335ms/step - loss: 0.2838 - binary_accuracy: 0.8880 - val_loss: 0.5366 - val_binary_accuracy: 0.7585\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 18s 337ms/step - loss: 0.1324 - binary_accuracy: 0.9473 - val_loss: 0.6267 - val_binary_accuracy: 0.7440\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94       161\n",
      "           1       0.82      0.72      0.77        46\n",
      "\n",
      "    accuracy                           0.90       207\n",
      "   macro avg       0.87      0.84      0.85       207\n",
      "weighted avg       0.90      0.90      0.90       207\n",
      "\n",
      "Starting Fold:  3\n",
      "Training model with https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 18s 345ms/step - loss: 0.4727 - binary_accuracy: 0.8293 - val_loss: 0.7764 - val_binary_accuracy: 0.5990\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 17s 335ms/step - loss: 0.2629 - binary_accuracy: 0.8965 - val_loss: 0.7117 - val_binary_accuracy: 0.6522\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 17s 336ms/step - loss: 0.1272 - binary_accuracy: 0.9522 - val_loss: 0.9689 - val_binary_accuracy: 0.6667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94       166\n",
      "           1       0.84      0.63      0.72        41\n",
      "\n",
      "    accuracy                           0.90       207\n",
      "   macro avg       0.88      0.80      0.83       207\n",
      "weighted avg       0.90      0.90      0.90       207\n",
      "\n",
      "Starting Fold:  4\n",
      "Training model with https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 18s 349ms/step - loss: 0.6681 - binary_accuracy: 0.7203 - val_loss: 1.5755 - val_binary_accuracy: 0.3478\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 18s 343ms/step - loss: 0.3533 - binary_accuracy: 0.8711 - val_loss: 1.3120 - val_binary_accuracy: 0.3671\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 18s 347ms/step - loss: 0.2303 - binary_accuracy: 0.9074 - val_loss: 1.0586 - val_binary_accuracy: 0.6232\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 18s 345ms/step - loss: 0.1192 - binary_accuracy: 0.9588 - val_loss: 0.9276 - val_binary_accuracy: 0.6957\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 18s 347ms/step - loss: 0.0531 - binary_accuracy: 0.9812 - val_loss: 1.6106 - val_binary_accuracy: 0.6135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94       172\n",
      "           1       0.86      0.51      0.64        35\n",
      "\n",
      "    accuracy                           0.90       207\n",
      "   macro avg       0.88      0.75      0.79       207\n",
      "weighted avg       0.90      0.90      0.89       207\n",
      "\n",
      "Starting Fold:  5\n",
      "Training model with https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 19s 365ms/step - loss: 0.5256 - binary_accuracy: 0.7948 - val_loss: 0.2388 - val_binary_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 18s 344ms/step - loss: 0.3667 - binary_accuracy: 0.8444 - val_loss: 0.2638 - val_binary_accuracy: 0.8986\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       156\n",
      "           1       0.72      0.61      0.66        51\n",
      "\n",
      "    accuracy                           0.85       207\n",
      "   macro avg       0.80      0.77      0.78       207\n",
      "weighted avg       0.84      0.85      0.84       207\n",
      "\n",
      "Starting Fold:  6\n",
      "Training model with https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 18s 340ms/step - loss: 0.4993 - binary_accuracy: 0.7978 - val_loss: 0.0965 - val_binary_accuracy: 0.9903\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 18s 337ms/step - loss: 0.3063 - binary_accuracy: 0.8705 - val_loss: 0.1143 - val_binary_accuracy: 0.9758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91       156\n",
      "           1       0.81      0.57      0.67        51\n",
      "\n",
      "    accuracy                           0.86       207\n",
      "   macro avg       0.84      0.76      0.79       207\n",
      "weighted avg       0.86      0.86      0.85       207\n",
      "\n",
      "Starting Fold:  7\n",
      "Training model with https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 18s 338ms/step - loss: 0.5053 - binary_accuracy: 0.8016 - val_loss: 0.0989 - val_binary_accuracy: 0.9903\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 17s 331ms/step - loss: 0.3036 - binary_accuracy: 0.8724 - val_loss: 0.1574 - val_binary_accuracy: 0.9660\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90       156\n",
      "           1       0.71      0.69      0.70        51\n",
      "\n",
      "    accuracy                           0.86       207\n",
      "   macro avg       0.81      0.80      0.80       207\n",
      "weighted avg       0.85      0.86      0.85       207\n",
      "\n",
      "Starting Fold:  8\n",
      "Training model with https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 18s 339ms/step - loss: 0.6212 - binary_accuracy: 0.7187 - val_loss: 0.1663 - val_binary_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 17s 331ms/step - loss: 0.3397 - binary_accuracy: 0.8548 - val_loss: 0.1797 - val_binary_accuracy: 0.9417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92       156\n",
      "           1       0.79      0.73      0.76        51\n",
      "\n",
      "    accuracy                           0.88       207\n",
      "   macro avg       0.85      0.83      0.84       207\n",
      "weighted avg       0.88      0.88      0.88       207\n",
      "\n",
      "Starting Fold:  9\n",
      "Training model with https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 18s 341ms/step - loss: 0.6160 - binary_accuracy: 0.7362 - val_loss: 0.2782 - val_binary_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 17s 332ms/step - loss: 0.4835 - binary_accuracy: 0.7961 - val_loss: 0.1097 - val_binary_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 17s 333ms/step - loss: 0.2893 - binary_accuracy: 0.8717 - val_loss: 0.1093 - val_binary_accuracy: 0.9806\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 17s 336ms/step - loss: 0.1482 - binary_accuracy: 0.9425 - val_loss: 0.0261 - val_binary_accuracy: 0.9903\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 17s 331ms/step - loss: 0.0684 - binary_accuracy: 0.9734 - val_loss: 0.0447 - val_binary_accuracy: 0.9903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93       156\n",
      "           1       0.84      0.73      0.78        51\n",
      "\n",
      "    accuracy                           0.90       207\n",
      "   macro avg       0.88      0.84      0.86       207\n",
      "weighted avg       0.90      0.90      0.90       207\n",
      "\n",
      "Starting Fold:  10\n",
      "Training model with https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 18s 342ms/step - loss: 0.5316 - binary_accuracy: 0.7901 - val_loss: 0.2016 - val_binary_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 18s 342ms/step - loss: 0.3544 - binary_accuracy: 0.8433 - val_loss: 0.2098 - val_binary_accuracy: 0.9757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90       156\n",
      "           1       0.69      0.69      0.69        51\n",
      "\n",
      "    accuracy                           0.85       207\n",
      "   macro avg       0.79      0.79      0.79       207\n",
      "weighted avg       0.85      0.85      0.85       207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "fold_no = 1\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "for train_index, test_index in KFold(10).split(initial_train):\n",
    "    print(\"Starting Fold: \", fold_no)\n",
    "    \n",
    "    x_train, x_val = initial_train[train_index], initial_train[test_index]\n",
    "    #print(initial_train)\n",
    "    \n",
    "    train = pd.DataFrame(data=x_train, index=[i for i in range(x_train.shape[0])], columns=[ \"comment\", \"target\"])\n",
    "     \n",
    "    val = pd.DataFrame(data=x_val, index=[i for i in range(x_val.shape[0])], columns=[\"comment\", \"target\"])\n",
    "    \n",
    "    x_train, x_test= train_test_split(x_train,  test_size=0.11115, random_state=125)\n",
    "\n",
    "    train = pd.DataFrame(data=x_train, index=[i for i in range(x_train.shape[0])], columns=[\"comment\", \"target\"])\n",
    "    test = pd.DataFrame(data=x_test, index=[i for i in range(x_test.shape[0])], columns=[\"comment\", \"target\"])\n",
    "    \n",
    "    #print(test)\n",
    "    \n",
    "    #ptrain=train\n",
    "    \n",
    "    data_batch_size=32\n",
    "    train.fillna(value='', inplace=True)\n",
    "    \n",
    "    train['target'] = np.where(train['target']== 0,0,1)\n",
    "     \n",
    "    ptrain_df = df_to_dataset(train, batch_size=data_batch_size)\n",
    "    \n",
    "    \n",
    "    test.fillna(value='', inplace=True)\n",
    "    test['target'] = np.where(test['target']== 0,0,1)\n",
    "     \n",
    "    ptest_df = df_to_dataset(test, batch_size=data_batch_size)\n",
    "    \n",
    "    \n",
    "    #test['target'] = np.where(test['is_toxic']== 0,0,1)\n",
    "     \n",
    "    #test_df = df_to_dataset(test, batch_size=data_batch_size)\n",
    "    \n",
    "    val.fillna(value='', inplace=True)\n",
    "    val['target'] = np.where(val['target']== 0,0,1)\n",
    "     \n",
    "    pval_df = df_to_dataset(val, batch_size=data_batch_size)\n",
    "    \n",
    "    \n",
    "    #AUTOTUNE = tf.data.AUTOTUNE\n",
    "    #train_dp = ptrain_df.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    #val_dp = pval_df.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    #test_dp = ptest_df.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    train_dp=ptrain_df\n",
    "    val_dp = pval_df\n",
    "    test_dp = ptest_df\n",
    "    \n",
    "    ##load model\n",
    "    classifier_model= build_classifier_model()\n",
    "\n",
    "    \n",
    "    ###start train\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = tf.metrics.BinaryAccuracy()\n",
    "    \n",
    "    \n",
    "    epochs = 10\n",
    "    steps_per_epoch = tf.data.experimental.cardinality(train_dp).numpy()\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "    init_lr = 3e-5\n",
    "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')\n",
    "    \n",
    "    classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    \n",
    "    ##set early stopping \n",
    "    callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
    "    \n",
    "    ##training started\n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    history = classifier_model.fit(x=train_dp,\n",
    "                               validation_data=val_dp,\n",
    "                               epochs=epochs,  callbacks=[callbacks])\n",
    "    \n",
    "    ##testing_model\n",
    "    y_tst=test['target'].values\n",
    "    y_pred = tf.sigmoid(classifier_model(tf.constant(test[\"comment\"])))\n",
    "    y_pred = (y_pred >= 0.5)\n",
    "    \n",
    "    \n",
    "    #print(y_tst, y_pred)\n",
    "    ##classification metrics\n",
    "    from sklearn import metrics\n",
    "    print(metrics.classification_report(y_tst, y_pred))\n",
    "    \n",
    "    bert_precision = precision_score(y_tst, y_pred, pos_label=1)\n",
    "    bert_recall = recall_score(y_tst, y_pred, pos_label=1)\n",
    "    bert_f1score = f1_score(y_tst, y_pred, pos_label=1)\n",
    "    bert_accuracy = accuracy_score(y_tst, y_pred)\n",
    "\n",
    "    bert_run_accuracy.append(bert_accuracy)\n",
    "    bert_run_f1score.append(bert_f1score)\n",
    "    bert_run_precision.append(bert_precision)\n",
    "    bert_run_recall.append(bert_recall)\n",
    "    \n",
    "    fold_no = fold_no+1\n",
    "\n",
    "    \n",
    "    #print(len(train))\n",
    "    #print(len(val))\n",
    "    #print(len(test))\n",
    "    #indexs=[train_index]\n",
    "    #val_indexes=[test_index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-crystal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
