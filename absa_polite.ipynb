{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel('politeness_finalset_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment=data['comment']\n",
    "rating=data['final_agreed_rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['polite', 'impolite', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = {\n",
    "    'impolite': 0,\n",
    "    'neutral': 1,\n",
    "    'polite': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [encoding[x] for x in rating]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train_comment, test_comment, train_target, test_target= train_test_split(comment, target, test_size=0.1, random_state=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_comment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "import spacy\n",
    "import heapq\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "#from gensim.matutils import softcossim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchtext.vocab as torchvocab\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
    "    x = (np.ones(maxlen) * value).astype(dtype)\n",
    "    if truncating == 'pre':\n",
    "        trunc = sequence[-maxlen:]\n",
    "    else:\n",
    "        trunc = sequence[:maxlen]\n",
    "    trunc = np.asarray(trunc, dtype=dtype)\n",
    "    if padding == 'post':\n",
    "        x[:len(trunc)] = trunc\n",
    "    else:\n",
    "        x[-len(trunc):] = trunc\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 1\n",
    "\n",
    "    def fit_on_text(self, text):\n",
    "        words = preprocess(text)\n",
    "        for word in words:\n",
    "            if word not in self.word2idx:\n",
    "                self.word2idx[word] = self.idx\n",
    "                self.idx2word[self.idx] = word\n",
    "                self.idx += 1\n",
    "\n",
    "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
    "        words = preprocess(text)\n",
    "        unknownidx = len(self.word2idx)+1\n",
    "        sequence = [self.word2idx[w] if w in self.word2idx else unknownidx for w in words]\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 70\n",
    "bert_dim = 768 \n",
    "polarities_dim = 3\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer(dataset, max_seq_len):\n",
    "    tokenizer = Tokenizer(max_seq_len)\n",
    "    for data in dataset:\n",
    "        tokenizer.fit_on_text(data)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/amiangshu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=build_tokenizer(dataset = train_comment, max_seq_len = max_seq_len)\n",
    " \n",
    "word2idx = train_tokenizer.word2idx\n",
    "\n",
    "\n",
    "#test_tokenizer=build_tokenizer(dataset = test_comment, max_seq_len = max_seq_len)\n",
    "\n",
    "#test_word2idx = test_tokenizer.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = build_tokenizer(dataset = comment, max_seq_len = max_seq_len)\n",
    "#word2idx = tokenizer.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, comments, targets, sentence_tokenizer):\n",
    "        \n",
    "        all_data = []\n",
    "        i=0\n",
    "        #c=comments[i]\n",
    "        #print(c)\n",
    "        for i in range(len(comments)):\n",
    "            \n",
    "            com = comments[i]\n",
    "            #print(i)\n",
    "            comment_indices = sentence_tokenizer.text_to_sequence(com)\n",
    "            rating = targets[i]\n",
    "            \n",
    "            newdata = {\n",
    "                     'comment_indices': comment_indices,\n",
    "                    'rating': rating,\n",
    "                     }\n",
    "            all_data.append(newdata)\n",
    "\n",
    "        self.data = all_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_Data = CreateDataset(comment, target, tokenizer)\n",
    "\n",
    "trainset= total_Data[1:409] \n",
    "testset=total_Data[410:500]\n",
    "val_set=  total_Data[501:588]\n",
    "\n",
    "#test=testset(frac=0.1)\n",
    "#testset= CreateDataset(comment, target, test_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_data_loader = DataLoader(dataset=trainset, batch_size=10, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "val_data_loader = DataLoader(dataset=val_set, batch_size=10, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "test_data_loader = DataLoader(dataset=testset, batch_size=10, shuffle=True, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABSABert(nn.Module):\n",
    "    def __init__(self, bert, bert_dim, polarities_dim, dropout):\n",
    "        \n",
    "        super(ABSABert, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        self.dense = nn.Linear(bert_dim, polarities_dim)\n",
    "        \n",
    "    def forward(self, review_indices):\n",
    "        \n",
    "        # embedding \n",
    "        text_bert_indices = review_indices        # batch_size x seq_len\n",
    "        _, pooled_output = self.bert(text_bert_indices,output_all_encoded_layers=False)\n",
    "        \n",
    "        # text_len = torch.sum(text_bert_indices != 0, dim=-1)\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        logits = self.dense(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = ABSABert(bert,bert_dim,polarities_dim,dropout)\n",
    "mymodel = mymodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_params(mymodel):\n",
    "    for child in mymodel.children():\n",
    "        for p in child.parameters():\n",
    "            if p.requires_grad:\n",
    "                stdv = 1. / math.sqrt(p.shape[0])\n",
    "                torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
    "    print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show parameters\n",
    "def paramsshow(net):\n",
    "    print(net)\n",
    "    params = list(net.parameters())\n",
    "    print(\"lenghth of parameters:\",len(params))\n",
    "    for name,parameters in net.named_parameters():\n",
    "        print(name,':',parameters.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "logdir = 'log'\n",
    "learning_rate = 0.01\n",
    "l2reg = 0.01\n",
    "log_step = 10\n",
    "lambbda = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "params = filter(lambda p: p.requires_grad, mymodel.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr= learning_rate, weight_decay=l2reg)\n",
    "num_epoch = 20\n",
    "reset_params(mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, criterion, optimizer, log_step, num_epoch, lambbda, device):\n",
    "    \n",
    "    print('#########Start Training#########')\n",
    "    accuracy_point = []\n",
    "    global_step = 0\n",
    "    \n",
    "    # loop over the dataset multiple times\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        print('>' * 50)\n",
    "        print('epoch:', epoch)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        correct = 0\n",
    "        accuracy = 0\n",
    "        total = 0\n",
    "        \n",
    "        newaccuracy = 0\n",
    "        newtotal = 0\n",
    "        newcorrect = 0\n",
    "        \n",
    "        # switch model to training mode\n",
    "        model.train()\n",
    "        \n",
    "        for i_batch, sample_batched in enumerate(data_loader):\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            losses = []\n",
    "            # switch model to training mode, clear gradient accumulators\n",
    "            mymodel.train()\n",
    "            optimizer.zero_grad()      \n",
    "        \n",
    "            comment_indices = sample_batched['comment_indices'].to(device)\n",
    "            rating = sample_batched['rating'].to(device)\n",
    "\n",
    "            output = mymodel(comment_indices) \n",
    "            \n",
    "            # losses\n",
    "            loss = criterion(output, rating)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculate running loss \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # get accuracy \n",
    "            total += rating.size(0)\n",
    "            correct += (torch.argmax(output, -1) == rating).sum().item()                       \n",
    "            newcorrect = correct\n",
    "            newtotal = total\n",
    "            newaccuracy = 100*newcorrect/newtotal\n",
    "            \n",
    "            if global_step %  log_step == 0:\n",
    "                \n",
    "                # print loss and accyracy\n",
    "                print ('[%2d, %2d] loss: %.3f accuracy: %.2f' %(epoch + 1, i_batch + 1, running_loss,newaccuracy))\n",
    "                running_loss = 0.0\n",
    "                newtotal = 0\n",
    "                newcorrect = 0\n",
    "                newaccuracy = 0\n",
    "                       \n",
    "        accuracy = 100 * correct / total\n",
    "        print ('epoch: %d, accuracy: %.2f' %(epoch,accuracy))\n",
    "        accuracy_point.append(accuracy)\n",
    "        \n",
    "    torch.save(mymodel, 'Bert.pkl')\n",
    "    print('#########Finished Training#########')\n",
    "    \n",
    "    return accuracy_point   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########Start Training#########\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "[ 1, 10] loss: 113.517 accuracy: 36.00\n",
      "[ 1, 20] loss: 41.837 accuracy: 43.00\n",
      "[ 1, 30] loss: 48.209 accuracy: 43.67\n",
      "[ 1, 40] loss: 35.311 accuracy: 46.00\n",
      "epoch: 0, accuracy: 46.00\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 1\n",
      "[ 2, 10] loss: 20.296 accuracy: 49.00\n",
      "[ 2, 20] loss: 13.204 accuracy: 44.50\n",
      "[ 2, 30] loss: 11.133 accuracy: 45.33\n",
      "[ 2, 40] loss: 14.261 accuracy: 45.25\n",
      "epoch: 1, accuracy: 45.25\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 2\n",
      "[ 3, 10] loss: 10.432 accuracy: 44.00\n",
      "[ 3, 20] loss: 11.707 accuracy: 50.00\n",
      "[ 3, 30] loss: 10.025 accuracy: 49.00\n",
      "[ 3, 40] loss: 11.966 accuracy: 48.00\n",
      "epoch: 2, accuracy: 48.00\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 3\n",
      "[ 4, 10] loss: 9.535 accuracy: 51.00\n",
      "[ 4, 20] loss: 10.107 accuracy: 56.00\n",
      "[ 4, 30] loss: 11.485 accuracy: 51.33\n",
      "[ 4, 40] loss: 9.298 accuracy: 50.25\n",
      "epoch: 3, accuracy: 50.25\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 4\n",
      "[ 5, 10] loss: 10.515 accuracy: 64.00\n",
      "[ 5, 20] loss: 11.139 accuracy: 58.00\n",
      "[ 5, 30] loss: 11.322 accuracy: 56.33\n",
      "[ 5, 40] loss: 11.747 accuracy: 54.00\n",
      "epoch: 4, accuracy: 54.00\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 5\n",
      "[ 6, 10] loss: 11.337 accuracy: 39.00\n",
      "[ 6, 20] loss: 10.315 accuracy: 47.00\n",
      "[ 6, 30] loss: 8.607 accuracy: 52.00\n",
      "[ 6, 40] loss: 10.600 accuracy: 48.50\n",
      "epoch: 5, accuracy: 48.50\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 6\n",
      "[ 7, 10] loss: 11.326 accuracy: 44.00\n",
      "[ 7, 20] loss: 12.081 accuracy: 47.50\n",
      "[ 7, 30] loss: 10.758 accuracy: 48.33\n",
      "[ 7, 40] loss: 8.530 accuracy: 50.50\n",
      "epoch: 6, accuracy: 50.50\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 7\n",
      "[ 8, 10] loss: 9.844 accuracy: 45.00\n",
      "[ 8, 20] loss: 9.531 accuracy: 49.00\n",
      "[ 8, 30] loss: 9.059 accuracy: 53.33\n",
      "[ 8, 40] loss: 9.636 accuracy: 53.75\n",
      "epoch: 7, accuracy: 53.75\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 8\n",
      "[ 9, 10] loss: 9.481 accuracy: 60.00\n",
      "[ 9, 20] loss: 8.953 accuracy: 57.00\n",
      "[ 9, 30] loss: 8.682 accuracy: 57.33\n",
      "[ 9, 40] loss: 8.685 accuracy: 55.75\n",
      "epoch: 8, accuracy: 55.75\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 9\n",
      "[10, 10] loss: 8.984 accuracy: 51.00\n",
      "[10, 20] loss: 9.408 accuracy: 49.50\n",
      "[10, 30] loss: 8.881 accuracy: 53.00\n",
      "[10, 40] loss: 9.005 accuracy: 55.00\n",
      "epoch: 9, accuracy: 55.00\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 10\n",
      "[11, 10] loss: 8.676 accuracy: 58.00\n",
      "[11, 20] loss: 9.447 accuracy: 56.00\n",
      "[11, 30] loss: 8.663 accuracy: 54.33\n",
      "[11, 40] loss: 8.522 accuracy: 56.50\n",
      "epoch: 10, accuracy: 56.50\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 11\n",
      "[12, 10] loss: 9.103 accuracy: 58.00\n",
      "[12, 20] loss: 9.277 accuracy: 55.00\n",
      "[12, 30] loss: 9.255 accuracy: 51.67\n",
      "[12, 40] loss: 8.683 accuracy: 53.75\n",
      "epoch: 11, accuracy: 53.75\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 12\n",
      "[13, 10] loss: 9.503 accuracy: 57.00\n",
      "[13, 20] loss: 8.349 accuracy: 60.00\n",
      "[13, 30] loss: 8.345 accuracy: 60.33\n",
      "[13, 40] loss: 9.592 accuracy: 57.25\n",
      "epoch: 12, accuracy: 57.25\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 13\n",
      "[14, 10] loss: 9.399 accuracy: 37.00\n",
      "[14, 20] loss: 8.599 accuracy: 49.50\n",
      "[14, 30] loss: 8.271 accuracy: 54.00\n",
      "[14, 40] loss: 9.297 accuracy: 53.75\n",
      "epoch: 13, accuracy: 53.75\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 14\n",
      "[15, 10] loss: 8.856 accuracy: 55.00\n",
      "[15, 20] loss: 8.923 accuracy: 58.00\n",
      "[15, 30] loss: 8.590 accuracy: 58.33\n",
      "[15, 40] loss: 8.916 accuracy: 57.75\n",
      "epoch: 14, accuracy: 57.75\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 15\n",
      "[16, 10] loss: 9.166 accuracy: 50.00\n",
      "[16, 20] loss: 9.356 accuracy: 51.00\n",
      "[16, 30] loss: 8.415 accuracy: 53.67\n",
      "[16, 40] loss: 8.510 accuracy: 55.75\n",
      "epoch: 15, accuracy: 55.75\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 16\n",
      "[17, 10] loss: 9.820 accuracy: 53.00\n",
      "[17, 20] loss: 8.864 accuracy: 54.00\n",
      "[17, 30] loss: 8.260 accuracy: 57.67\n",
      "[17, 40] loss: 8.954 accuracy: 57.50\n",
      "epoch: 16, accuracy: 57.50\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 17\n",
      "[18, 10] loss: 8.520 accuracy: 62.00\n",
      "[18, 20] loss: 8.396 accuracy: 61.50\n",
      "[18, 30] loss: 8.678 accuracy: 60.33\n",
      "[18, 40] loss: 9.602 accuracy: 57.50\n",
      "epoch: 17, accuracy: 57.50\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 18\n",
      "[19, 10] loss: 8.762 accuracy: 60.00\n",
      "[19, 20] loss: 8.900 accuracy: 58.50\n",
      "[19, 30] loss: 8.383 accuracy: 59.00\n",
      "[19, 40] loss: 9.329 accuracy: 58.50\n",
      "epoch: 18, accuracy: 58.50\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 19\n",
      "[20, 10] loss: 8.228 accuracy: 61.00\n",
      "[20, 20] loss: 9.401 accuracy: 57.00\n",
      "[20, 30] loss: 8.981 accuracy: 56.67\n",
      "[20, 40] loss: 8.527 accuracy: 58.25\n",
      "epoch: 19, accuracy: 58.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amiangshu/anaconda3/envs/polite/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ABSABert. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########Finished Training#########\n"
     ]
    }
   ],
   "source": [
    "accuracy_point = train(mymodel, train_data_loader, criterion, optimizer, log_step, num_epoch, lambbda, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc_f1(test_data_loader,model):\n",
    "    n_test_correct, n_test_total = 0, 0\n",
    "    t_targets_all, t_outputs_all = None, None\n",
    "    \n",
    "    # switch model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t_batch, t_sample_batched in enumerate(test_data_loader):\n",
    "            t_sentence = t_sample_batched['comment_indices'].to(device)\n",
    "            \n",
    "            #print(t_sentence)\n",
    "            t_targets = t_sample_batched['rating'].to(device)\n",
    "        \n",
    "            t_overall = model(t_sentence) \n",
    "            \n",
    "            n_test_correct += (torch.argmax(t_overall, -1) == t_targets).sum().item()\n",
    "            n_test_total += len(t_overall)\n",
    "\n",
    "            if t_targets_all is None:\n",
    "                t_targets_all = t_targets\n",
    "                t_outputs_all = t_overall\n",
    "            else:\n",
    "                t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
    "                t_outputs_all = torch.cat((t_outputs_all, t_overall), dim=0)\n",
    "\n",
    "        test_acc = n_test_correct / n_test_total\n",
    "        f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
    "    \n",
    "        print ('test_acc: %.4f, f1: %.4f' %(test_acc,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = torch.load('Bert.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.5000, f1: 0.2222\n"
     ]
    }
   ],
   "source": [
    "evaluate_acc_f1(test_data_loader,newmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
